{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment : 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n",
    "2. Load the data in your notebook.\n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum.\n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n",
    "8. Make sure that you are using GPU to train the given models.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can use gdown modules to import dataset for the assignment\n",
    "#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n",
    "#you can run the below cell to import the required preprocessed data.csv file and glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n",
    "#!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Model-1 </font>\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "input_layer = Input(shape=(n,))\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "flatten = Flatten()(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "\n",
       "    clean_categories                 clean_subcategories  \\\n",
       "0       math_science  appliedsciences health_lifescience   \n",
       "1       specialneeds                        specialneeds   \n",
       "2  literacy_language                            literacy   \n",
       "3    appliedlearning                    earlydevelopment   \n",
       "4  literacy_language                            literacy   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  \n",
       "2  having class 24 students comes diverse learner...  329.00  \n",
       "3  i recently read article giving students choice...  481.04  \n",
       "4  my students crave challenge eat obstacles brea...   17.74  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
       "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
       "       'clean_categories', 'clean_subcategories', 'essay', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
       "       'teacher_number_of_previously_posted_projects', 'clean_categories',\n",
       "       'clean_subcategories', 'essay', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  = df[\"project_is_approved\"]\n",
    "X = df.drop(['project_is_approved'],axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform stratified train test split on the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the data is already preprocessed, we can directly move to vectorization part\n",
    "#first we will vectorize the text data\n",
    "#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n",
    "# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "# after text vectorization you should get train_padded_docs and test_padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109248\n"
     ]
    }
   ],
   "source": [
    "texts = X['essay'].to_list()\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "essay_lengths = []\n",
    "for essay in texts:\n",
    "    essay_lengths.append(len(essay.split(' ')))  \n",
    "sns.histplot(essay_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.percentile(essay_lengths,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56381"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.word_index\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sequence = tokenizer.texts_to_sequences(texts)\n",
    "padded_data = tf.keras.preprocessing.sequence.pad_sequences(text_to_sequence,padding='post',maxlen=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,   996,   297,    18,  2585,  3677,   265,   733,     6,\n",
       "          66,   265,  1418,     1,   275,  2381,     2,    24,    19,\n",
       "        1488,  3727,   265,   733,     6,   371,     3,    30,    46,\n",
       "        1202,   200,   265,   188,     4,     1,    22,   118,   140,\n",
       "         448,    68,   402,   710,    12,     8,   243,    98,    65,\n",
       "         469,   733,    72,    42,    46,   311,   127,     1,   271,\n",
       "        1558,     2,   237,   286,    65,   265,   720,   136,     2,\n",
       "          24,    18,   733,  1516,    11,  1241,    65,   199,   200,\n",
       "         443,   146,     2,  2586,   733,   394,    68,   309,  7229,\n",
       "        1241,   450,    49,   600,   733,    53,  3607,  2709,  5311,\n",
       "       18004, 10927, 15101,     7,  1419,  1511,   475,   371,     3,\n",
       "          30,     2,  1488,   733,  2514,  1348,  4484,  4905,  6569,\n",
       "         924,     2,   144,    40,  1511,     8,   103,   375,     2,\n",
       "         147,   313,    58,    70,   313,    42,     7,   733,    72,\n",
       "         544,   330,   653,   188,   405,     1,    65,    54,   496,\n",
       "         208,    50,   406,    65,    73,    38,   733,    72,    42,\n",
       "           2,    15,    46,     1,    65,    73,   165,   410,   238,\n",
       "           6,   551,     2,   111,    53,  2514,     6,     2,     8,\n",
       "         103,    18,  1096,     7,   733,    46,   313,   650,    42,\n",
       "         241,    18,   581,    58,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 230)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n",
    "# we will be storing this data in form of an embedding matrix and will use it while defining our model\n",
    "# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand how to create embedding matrix\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    glove_vec = pickle.load(f)\n",
    "    glove_words =  set(glove_vec.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51510"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vec.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6744 0.8803852361611181\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "word_dims = 300\n",
    "\n",
    "embeding = np.zeros((vocab_size,word_dims))\n",
    "\n",
    "count = 0\n",
    "not_found = [] \n",
    "for word,idx in vocab.items():# There is not item with idx 0 in vocab dict\n",
    "    if glove_vec.get(word) is not None:\n",
    "        count+=1\n",
    "        embeding[idx] = glove_vec[word]\n",
    "    else:\n",
    "        not_found.append(word)\n",
    "print(len(vocab)-count, count/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56382, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hokki',\n",
       " 'ozobots',\n",
       " 'ozobot',\n",
       " 'storyworks',\n",
       " 'gonoodle',\n",
       " 'fitbits',\n",
       " 'mobymax',\n",
       " 'rekenreks',\n",
       " 'tiggly',\n",
       " 'breakoutedu',\n",
       " 'bloxels',\n",
       " 'newsela',\n",
       " 'zearn',\n",
       " '3doodlers',\n",
       " 'xtramath',\n",
       " 'magnatiles',\n",
       " 'hooki',\n",
       " 'plickers',\n",
       " 'makeys',\n",
       " 'rekenrek']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matrix is formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Categorical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
       "       'teacher_number_of_previously_posted_projects', 'clean_categories',\n",
       "       'clean_subcategories', 'essay', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['teacher_number_of_previously_posted_projects','price']\n",
    "categorical_features = ['school_state', 'project_grade_category','clean_categories','clean_subcategories','teacher_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 5/5 [00:00<00:00, 38.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature : school_state has 51\n",
      "Feature : project_grade_category has 4\n",
      "Feature : clean_categories has 51\n",
      "Feature : clean_subcategories has 401\n",
      "Feature : teacher_prefix has 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n",
    "# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n",
    "# label encoder gives an error for 'unseen values' (values present in test but not in train)\n",
    "# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n",
    "# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n",
    "# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "# after categorical feature vectorization you will have column_train_data and column_test_data.\n",
    "label_encoders = []\n",
    "feature_encoded = []\n",
    "for feature in tqdm(categorical_features):\n",
    "    encode = dict()\n",
    "    featurized = []\n",
    "    for idx,feat in enumerate(X[feature].unique()):\n",
    "        encode[feat] = idx\n",
    "    \n",
    "    label_encoders.append(encode)\n",
    "    \n",
    "    for cat in X[feature]:\n",
    "        featurized.append(encode[cat])\n",
    "    \n",
    "    feature_encoded.append(featurized)\n",
    "    print(f\"Feature : {feature} has {len(encode)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = np.array(feature_encoded).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Numerical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to standardise the numerical columns\n",
    "# stack both the numerical features\n",
    "#after numerical feature vectorization you will have numerical_data_train and numerical_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmean, pmean = X[numerical_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstd, pstd = X[numerical_features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_no_project = (X['teacher_number_of_previously_posted_projects'].to_numpy() - tmean) / tstd\n",
    "standard_price = (X['price'].to_numpy() - pmean) / pstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred = np.vstack([standard_no_project,standard_price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pred = num_pred.T\n",
    "num_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 230)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final =np.hstack([padded_data,cat_features[:,0].reshape(-1,1),cat_features[:,1].reshape(-1,1),cat_features[:,2].reshape(-1,1),\n",
    "                    cat_features[:,3].reshape(-1,1),cat_features[:,4].reshape(-1,1),num_pred])\n",
    "y_final = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 237)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_final,y_final,test_size=0.2,stratify=y_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of now we have vectorized all our features now we will define our model.\n",
    "# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n",
    "# Please go through - https://keras.io/guides/functional_api/\n",
    "# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n",
    "# while defining your model make sure that you use variable names while defining any length,dimension or size.\n",
    "#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n",
    "# the embedding layer for text data should be non trainable\n",
    "# the embedding layer for categorical data should be trainable\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n",
    "#print model.summary() after you have defined the model\n",
    "#plot the model using utils.plot_model module and make sure that it is similar to the above image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 23:24:34.369694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 23:24:35.017940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "in_len = padded_data.shape[1]\n",
    "in_cat_feat = 1\n",
    "num_feat = 2\n",
    "\n",
    "# Defining inputs\n",
    "essay_seq_input_layer = tf.keras.Input(shape=(in_len,),dtype= tf.int32)# The input will be a sequence \n",
    "student_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "project_grade_category_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "clean_categories_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "clean_subcategories_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "teacher_prefix_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype = tf.int32)\n",
    "numerical_input_layer = tf.keras.Input(shape=(num_feat,),dtype = tf.int32)\n",
    "\n",
    "\n",
    "#Defining embbeding layers\n",
    "essay_embbeding_layer = tf.keras.layers.Embedding(input_dim= vocab_size,output_dim = word_dims,weights=[embeding],\n",
    "                                            input_length=in_len, trainable=False)(essay_seq_input_layer)\n",
    "\n",
    "student_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[0])+1,output_dim = 5,\n",
    "                                            input_length=1)(student_input_layer)\n",
    "\n",
    "project_grade_category_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[1])+1,output_dim = 2,\n",
    "                                            input_length=1)(project_grade_category_input_layer)\n",
    "\n",
    "clean_categories_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[2])+1,output_dim = 5,\n",
    "                                            input_length=1)(clean_categories_input_layer)\n",
    "\n",
    "clean_subcategories_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[3])+1,output_dim = 5,\n",
    "                                            input_length=1)(clean_subcategories_input_layer)\n",
    "\n",
    "teacher_prefix_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[4])+1,output_dim = 2,\n",
    "                                            input_length=1)(teacher_prefix_input_layer)\n",
    "\n",
    "# LSTM on embbeding layer\n",
    "lstm = tf.keras.layers.LSTM(units = 20)(essay_embbeding_layer)\n",
    "\n",
    "# Flatten\n",
    "\n",
    "f1 = tf.keras.layers.Flatten()(lstm)\n",
    "f2 = tf.keras.layers.Flatten()(student_embbeding_layer)\n",
    "f3 = tf.keras.layers.Flatten()(project_grade_category_embbeding_layer)\n",
    "f4 = tf.keras.layers.Flatten()(clean_categories_embbeding_layer)\n",
    "f5 = tf.keras.layers.Flatten()(clean_subcategories_embbeding_layer)\n",
    "f6 = tf.keras.layers.Flatten()(teacher_prefix_embbeding_layer)\n",
    "\n",
    "# Dense for numerical\n",
    "f7 = tf.keras.layers.Dense(1,activation='sigmoid')(numerical_input_layer)\n",
    "\n",
    "# Concat\n",
    "concat = tf.keras.layers.concatenate([f1,f2,f3,f4,f5,f6,f7],axis=1)\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(512,activation='relu')(concat)\n",
    "\n",
    "dropout_1 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(16,activation='relu')(dropout_1)\n",
    "\n",
    "dropout_2 = tf.keras.layers.Dropout(0.3)(dense_2)\n",
    "\n",
    "dense_3 = tf.keras.layers.Dense(8,activation='relu')(dropout_2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense_3)\n",
    "\n",
    "model = tf.keras.Model([essay_seq_input_layer,\n",
    "                        student_input_layer,\n",
    "                        project_grade_category_input_layer,\n",
    "                        clean_categories_input_layer,\n",
    "                        clean_subcategories_input_layer,\n",
    "                        teacher_prefix_input_layer,\n",
    "                        numerical_input_layer\n",
    "                       ],\n",
    "                        output\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 230)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 230, 300)     16914600    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20)           25680       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 5)         260         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 2)         10          ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 5)         260         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 5)         2010        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 2)         12          ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 20)           0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 5)            0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2)            0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 5)            0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 5)            0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 2)            0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            3           ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 40)           0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          20992       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           8208        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 8)            136         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            9           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,972,180\n",
      "Trainable params: 57,580\n",
      "Non-trainable params: 16,914,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Compiling and fititng your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  1/683 [..............................] - ETA: 35:09 - loss: 0.6818 - binary_accuracy: 0.7891 - auc: 0.4650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 23:24:38.874679: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 19s 23ms/step - loss: 0.4290 - binary_accuracy: 0.8485 - auc: 0.5575 - val_loss: 0.4243 - val_binary_accuracy: 0.8486 - val_auc: 0.6009\n",
      "Epoch 2/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.4060 - binary_accuracy: 0.8486 - auc: 0.6539 - val_loss: 0.4029 - val_binary_accuracy: 0.8486 - val_auc: 0.7036\n",
      "Epoch 3/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3818 - binary_accuracy: 0.8487 - auc: 0.7171 - val_loss: 0.3773 - val_binary_accuracy: 0.8486 - val_auc: 0.7354\n",
      "Epoch 4/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3731 - binary_accuracy: 0.8488 - auc: 0.7356 - val_loss: 0.3747 - val_binary_accuracy: 0.8486 - val_auc: 0.7416\n",
      "Epoch 5/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3652 - binary_accuracy: 0.8513 - auc: 0.7533 - val_loss: 0.3740 - val_binary_accuracy: 0.8523 - val_auc: 0.7513\n",
      "Epoch 6/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3591 - binary_accuracy: 0.8532 - auc: 0.7654 - val_loss: 0.3747 - val_binary_accuracy: 0.8488 - val_auc: 0.7546\n",
      "Epoch 7/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3538 - binary_accuracy: 0.8555 - auc: 0.7743 - val_loss: 0.3879 - val_binary_accuracy: 0.8530 - val_auc: 0.7535\n",
      "Epoch 8/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3492 - binary_accuracy: 0.8569 - auc: 0.7823 - val_loss: 0.3776 - val_binary_accuracy: 0.8545 - val_auc: 0.7333\n",
      "Epoch 8: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a580a65e80>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define custom auc as metric , do not use tf.keras.metrics\n",
    "# https://stackoverflow.com/a/46844409 - custom AUC reference 1\n",
    "# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807  - custom AUC reference 2\n",
    "# compile and fit your model\n",
    "\n",
    "optimse = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.01, patience=3, verbose=1)\n",
    "model.compile(optimizer = optimse , loss='binary_crossentropy' , metrics= ['binary_accuracy' , tf.metrics.AUC(name='auc')])\n",
    "\n",
    "model.fit([X_train[:,:230],X_train[:,230:231],X_train[:,231:232],X_train[:,232:233],X_train[:,233:234],X_train[:,234:235],X_train[:,235:237]],\n",
    "          y_train,validation_data=([X_test[:,:230],X_test[:,230:231],X_test[:,231:232],X_test[:,232:233],X_test[:,233:234],X_test[:,234:235],X_test[:,235:237]],\n",
    "                                   y_test),batch_size = 128, epochs = 25,callbacks = [early_stop ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Fit TF-IDF vectorizer on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n",
    "\n",
    "3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \n",
    "frequent words and very very rare words don't give much information.\n",
    "Hint - A preferable IDF range is 2-11 for model 2. <br>\n",
    "4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\n",
    "sentence of train and test data and include only those features(words) which are present in the defined IDF range.\n",
    "5. Perform tokenization on the modified text data same as you have done for previous model.\n",
    "6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n",
    "7. Define the model, compile and fit the model.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "vect.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56345,\n",
       " array([ 7.18528456,  5.91178569, 11.90823778, ..., 11.50277267,\n",
       "        11.50277267, 11.90823778]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.vocabulary_),vect.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_vals = np.logical_and(vect.idf_>=2,vect.idf_<=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([32034, 24311]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(truth_vals,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i,truth_val in enumerate(truth_vals):\n",
    "    if truth_val:\n",
    "        idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 7, 9, 10, 15, 21, 25, 29, 41]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████| 56345/56345 [00:00<00:00, 1199622.64it/s]\n"
     ]
    }
   ],
   "source": [
    "lookup = set(idxs)\n",
    "count = 0\n",
    "tfidf_vocab = dict()\n",
    "for word,num in tqdm(vect.vocabulary_.items()):\n",
    "    if num in lookup:\n",
    "        tfidf_vocab[word] = count\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vocab = set(tfidf_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████| 109248/109248 [00:05<00:00, 18355.65it/s]\n"
     ]
    }
   ],
   "source": [
    "prep_text = []\n",
    "lengths = []\n",
    "for para in tqdm(texts):\n",
    "    words = para.split(\" \")\n",
    "    new_para = ''\n",
    "    length = 0\n",
    "    for word in words:\n",
    "        if word in tfidf_vocab:\n",
    "            new_para += word+' '\n",
    "            length+=1\n",
    "    lengths.append(length)\n",
    "    prep_text.append(new_para.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(lengths,99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(prep_text)\n",
    "vocab = tokenizer.word_index\n",
    "text_to_sequence = tokenizer.texts_to_sequences(prep_text)\n",
    "padded_data = tf.keras.preprocessing.sequence.pad_sequences(text_to_sequence,padding='post',maxlen=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 0.9874131051787257\n"
     ]
    }
   ],
   "source": [
    "len(vocab.keys())\n",
    "vocab_size = len(vocab) + 1\n",
    "word_dims = 300\n",
    "\n",
    "embeding = np.zeros((vocab_size,word_dims))\n",
    "\n",
    "count = 0\n",
    "not_found = [] \n",
    "for word,idx in vocab.items():# There is not item with idx 0 in vocab dict\n",
    "    if glove_vec.get(word) is not None:\n",
    "        count+=1\n",
    "        embeding[idx] = glove_vec[word]\n",
    "    else:\n",
    "        not_found.append(word)\n",
    "print(len(vocab)-count, count/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24312, 300)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_final =np.hstack([padded_data,cat_features[:,0].reshape(-1,1),cat_features[:,1].reshape(-1,1),cat_features[:,2].reshape(-1,1),\n",
    "                    cat_features[:,3].reshape(-1,1),cat_features[:,4].reshape(-1,1),num_pred])\n",
    "y_final = y.to_numpy()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_final,y_final,test_size=0.2,stratify=y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 22:52:03.251107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 22:52:06.555465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "in_len = padded_data.shape[1]\n",
    "in_cat_feat = 1\n",
    "num_feat = 2\n",
    "\n",
    "# Defining inputs\n",
    "essay_seq_input_layer = tf.keras.Input(shape=(in_len,),dtype= tf.int32)# The input will be a sequence \n",
    "student_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "project_grade_category_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "clean_categories_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "clean_subcategories_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype= tf.int32)\n",
    "teacher_prefix_input_layer = tf.keras.Input(shape=(in_cat_feat,),dtype = tf.int32)\n",
    "numerical_input_layer = tf.keras.Input(shape=(num_feat,),dtype = tf.int32)\n",
    "\n",
    "\n",
    "#Defining embbeding layers\n",
    "essay_embbeding_layer = tf.keras.layers.Embedding(input_dim= vocab_size,output_dim = word_dims,weights=[embeding],\n",
    "                                            input_length=in_len, trainable=False)(essay_seq_input_layer)\n",
    "\n",
    "student_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[0])+1,output_dim = 5,\n",
    "                                            input_length=1)(student_input_layer)\n",
    "\n",
    "project_grade_category_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[1])+1,output_dim = 2,\n",
    "                                            input_length=1)(project_grade_category_input_layer)\n",
    "\n",
    "clean_categories_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[2])+1,output_dim = 5,\n",
    "                                            input_length=1)(clean_categories_input_layer)\n",
    "\n",
    "clean_subcategories_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[3])+1,output_dim = 5,\n",
    "                                            input_length=1)(clean_subcategories_input_layer)\n",
    "\n",
    "teacher_prefix_embbeding_layer = tf.keras.layers.Embedding(input_dim= len(label_encoders[4])+1,output_dim = 2,\n",
    "                                            input_length=1)(teacher_prefix_input_layer)\n",
    "\n",
    "# LSTM on embbeding layer\n",
    "lstm = tf.keras.layers.LSTM(units = 20)(essay_embbeding_layer)\n",
    "\n",
    "# Flatten\n",
    "\n",
    "f1 = tf.keras.layers.Flatten()(lstm)\n",
    "f2 = tf.keras.layers.Flatten()(student_embbeding_layer)\n",
    "f3 = tf.keras.layers.Flatten()(project_grade_category_embbeding_layer)\n",
    "f4 = tf.keras.layers.Flatten()(clean_categories_embbeding_layer)\n",
    "f5 = tf.keras.layers.Flatten()(clean_subcategories_embbeding_layer)\n",
    "f6 = tf.keras.layers.Flatten()(teacher_prefix_embbeding_layer)\n",
    "\n",
    "# Dense for numerical\n",
    "f7 = tf.keras.layers.Dense(1,activation='sigmoid')(numerical_input_layer)\n",
    "\n",
    "# Concat\n",
    "concat = tf.keras.layers.concatenate([f1,f2,f3,f4,f5,f6,f7],axis=1)\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(512,activation='relu')(concat)\n",
    "\n",
    "dropout_1 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(16,activation='relu')(dropout_1)\n",
    "\n",
    "dropout_2 = tf.keras.layers.Dropout(0.3)(dense_2)\n",
    "\n",
    "dense_3 = tf.keras.layers.Dense(8,activation='relu')(dropout_2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense_3)\n",
    "\n",
    "model = tf.keras.Model([essay_seq_input_layer,\n",
    "                        student_input_layer,\n",
    "                        project_grade_category_input_layer,\n",
    "                        clean_categories_input_layer,\n",
    "                        clean_subcategories_input_layer,\n",
    "                        teacher_prefix_input_layer,\n",
    "                        numerical_input_layer\n",
    "                       ],\n",
    "                        output\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 230)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 230, 300)     7293600     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20)           25680       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 5)         260         ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 2)         10          ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 5)         260         ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 5)         2010        ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 2)         12          ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 20)           0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 5)            0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2)            0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 5)            0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 5)            0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 2)            0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            3           ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 40)           0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          20992       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           8208        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 8)            136         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            9           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,351,180\n",
      "Trainable params: 57,580\n",
      "Non-trainable params: 7,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 22:52:47.914363: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 20s 22ms/step - loss: 0.4433 - binary_accuracy: 0.8474 - auc: 0.5186 - val_loss: 0.4231 - val_binary_accuracy: 0.8486 - val_auc: 0.5581\n",
      "Epoch 2/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.4254 - binary_accuracy: 0.8486 - auc: 0.5552 - val_loss: 0.4227 - val_binary_accuracy: 0.8486 - val_auc: 0.5708\n",
      "Epoch 3/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.4215 - binary_accuracy: 0.8486 - auc: 0.5734 - val_loss: 0.4275 - val_binary_accuracy: 0.8486 - val_auc: 0.5658\n",
      "Epoch 4/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.4117 - binary_accuracy: 0.8486 - auc: 0.6325 - val_loss: 0.4099 - val_binary_accuracy: 0.8486 - val_auc: 0.6632\n",
      "Epoch 5/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3998 - binary_accuracy: 0.8481 - auc: 0.6682 - val_loss: 0.3966 - val_binary_accuracy: 0.8486 - val_auc: 0.6950\n",
      "Epoch 6/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3885 - binary_accuracy: 0.8486 - auc: 0.7037 - val_loss: 0.3920 - val_binary_accuracy: 0.8483 - val_auc: 0.7040\n",
      "Epoch 7/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3798 - binary_accuracy: 0.8486 - auc: 0.7208 - val_loss: 0.4005 - val_binary_accuracy: 0.8484 - val_auc: 0.7231\n",
      "Epoch 8/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3748 - binary_accuracy: 0.8497 - auc: 0.7342 - val_loss: 0.3806 - val_binary_accuracy: 0.8486 - val_auc: 0.7308\n",
      "Epoch 9/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3701 - binary_accuracy: 0.8506 - auc: 0.7438 - val_loss: 0.3783 - val_binary_accuracy: 0.8484 - val_auc: 0.7337\n",
      "Epoch 10/25\n",
      "683/683 [==============================] - 15s 22ms/step - loss: 0.3660 - binary_accuracy: 0.8521 - auc: 0.7514 - val_loss: 0.3809 - val_binary_accuracy: 0.8488 - val_auc: 0.7373\n",
      "Epoch 11/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3620 - binary_accuracy: 0.8538 - auc: 0.7605 - val_loss: 0.3786 - val_binary_accuracy: 0.8502 - val_auc: 0.7395\n",
      "Epoch 12/25\n",
      "683/683 [==============================] - 15s 21ms/step - loss: 0.3577 - binary_accuracy: 0.8564 - auc: 0.7680 - val_loss: 0.3837 - val_binary_accuracy: 0.8489 - val_auc: 0.7423\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553b0cbb670>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimse = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.01, patience=3, verbose=1)\n",
    "model.compile(optimizer = optimse , loss='binary_crossentropy' , metrics= ['binary_accuracy' , tf.metrics.AUC(name='auc')])\n",
    "\n",
    "model.fit([X_train[:,:230],X_train[:,230:231],X_train[:,231:232],X_train[:,232:233],X_train[:,233:234],X_train[:,234:235],X_train[:,235:237]],\n",
    "          y_train,validation_data=([X_test[:,:230],X_test[:,230:231],X_test[:,231:232],X_test[:,232:233],X_test[:,233:234],X_test[:,234:235],X_test[:,235:237]],\n",
    "                                   y_test),batch_size = 128, epochs = 25,callbacks = [early_stop ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this model you can use the text vectorized data from model1 \n",
    "#for other than text data consider the following steps\n",
    "# you have to perform one hot encoding of categorical features. You can use onehotencoder() or countvectorizer() for the same.\n",
    "# Stack up standardised numerical features and all the one hot encoded categorical features\n",
    "#the input to conv1d layer is 3d, you can convert your 2d data to 3d using np.newaxis\n",
    "# Note - deep learning models won't work with sparse features, you have to convert them to dense features before fitting in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Host encoding of other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
       "       'teacher_number_of_previously_posted_projects', 'clean_categories',\n",
       "       'clean_subcategories', 'essay', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "y  = df[\"project_is_approved\"]\n",
    "X = df.drop(['project_is_approved'],axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown = 'ignore',sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['school_state', 'project_grade_category','clean_categories','clean_subcategories','teacher_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_school_state = OneHotEncoder(handle_unknown = 'ignore',sparse=False)\n",
    "school_state_onehot = encoder_school_state.fit_transform(X['school_state'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_project_grade_category = OneHotEncoder(handle_unknown = 'ignore',sparse=False)\n",
    "project_grade_category_onehot = encoder_project_grade_category.fit_transform(X['project_grade_category'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clean_categories = OneHotEncoder(handle_unknown = 'ignore',sparse=False)\n",
    "clean_categories_onehot = encoder_clean_categories.fit_transform(X['clean_categories'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_clean_subcategories = OneHotEncoder(handle_unknown = 'ignore',sparse=False)\n",
    "clean_subcategories_onehot = encoder_clean_subcategories.fit_transform(X['clean_subcategories'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_teacher_prefix = OneHotEncoder(handle_unknown = 'ignore',sparse=False)\n",
    "teacher_prefix_onehot = encoder_teacher_prefix.fit_transform(X['teacher_prefix'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_feature = np.hstack((school_state_onehot,project_grade_category_onehot,clean_categories_onehot\n",
    "                           ,clean_subcategories_onehot,teacher_prefix_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 512)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109248\n"
     ]
    }
   ],
   "source": [
    "texts = X['essay'].to_list()\n",
    "print(len(texts))\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab = tokenizer.word_index\n",
    "text_to_sequence = tokenizer.texts_to_sequences(texts)\n",
    "padded_data = tf.keras.preprocessing.sequence.pad_sequences(text_to_sequence,padding='post',maxlen=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6744 0.8803852361611181\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "word_dims = 300\n",
    "\n",
    "embeding = np.zeros((vocab_size,word_dims))\n",
    "\n",
    "count = 0\n",
    "not_found = [] \n",
    "for word,idx in vocab.items():# There is not item with idx 0 in vocab dict\n",
    "    if glove_vec.get(word) is not None:\n",
    "        count+=1\n",
    "        embeding[idx] = glove_vec[word]\n",
    "    else:\n",
    "        not_found.append(word)\n",
    "print(len(vocab)-count, count/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 230)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.hstack((padded_data,other_feature))\n",
    "y_final = y.to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_final,y_final,test_size=0.2,stratify=y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 742)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = padded_data.shape[1]\n",
    "in_other_len = other_feature.shape[1]\n",
    "\n",
    "# Defining inputs\n",
    "essay_seq_input_layer = tf.keras.Input(shape=(in_len,),dtype= tf.int32)# The input will be a sequence \n",
    "other_inputs_layers = tf.keras.Input(shape=(in_other_len,1,))\n",
    "\n",
    "#Defining embbeding layers\n",
    "essay_embbeding_layer = tf.keras.layers.Embedding(input_dim= vocab_size,output_dim = word_dims,weights=[embeding],\n",
    "                                            input_length=in_len, trainable=False)(essay_seq_input_layer)\n",
    "\n",
    "conv_1d_layer_1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 5, strides = 3, activation = \"relu\")(other_inputs_layers)\n",
    "conv_1d_layer_2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 5, strides = 1, activation = \"relu\")(conv_1d_layer_1)\n",
    "\n",
    "\n",
    "# LSTM on embbeding layer\n",
    "lstm = tf.keras.layers.LSTM(units = 20)(essay_embbeding_layer)\n",
    "\n",
    "# Flatten\n",
    "\n",
    "f1 = tf.keras.layers.Flatten()(lstm)\n",
    "f2 = tf.keras.layers.Flatten()(conv_1d_layer_2)\n",
    "\n",
    "# Concat\n",
    "concat = tf.keras.layers.concatenate([f1,f2],axis=1)\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(64,activation='relu')(concat)\n",
    "\n",
    "dropout_1 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(16,activation='relu')(dropout_1)\n",
    "\n",
    "dropout_2 = tf.keras.layers.Dropout(0.3)(dense_2)\n",
    "\n",
    "dense_3 = tf.keras.layers.Dense(8,activation='relu')(dropout_2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense_3)\n",
    "\n",
    "model_3 = tf.keras.Model([essay_seq_input_layer,\n",
    "                        other_inputs_layers\n",
    "                       ],\n",
    "                        output\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 230)]        0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 512, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 230, 300)     16914600    ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 170, 64)      384         ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 20)           25680       ['embedding_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 166, 64)      20544       ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 20)           0           ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 10624)        0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 10644)        0           ['flatten_8[0][0]',              \n",
      "                                                                  'flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           681280      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           1040        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 8)            136         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            9           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,643,673\n",
      "Trainable params: 729,073\n",
      "Non-trainable params: 16,914,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 00:48:49.356343: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 20s 21ms/step - loss: 0.4462 - binary_accuracy: 0.8443 - auc: 0.5284 - val_loss: 0.4387 - val_binary_accuracy: 0.8486 - val_auc: 0.5809\n",
      "Epoch 2/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.4258 - binary_accuracy: 0.8486 - auc: 0.5582 - val_loss: 0.4288 - val_binary_accuracy: 0.8486 - val_auc: 0.5760\n",
      "Epoch 3/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.3970 - binary_accuracy: 0.8483 - auc: 0.6781 - val_loss: 0.3983 - val_binary_accuracy: 0.8486 - val_auc: 0.7255\n",
      "Epoch 4/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.3784 - binary_accuracy: 0.8485 - auc: 0.7227 - val_loss: 0.3821 - val_binary_accuracy: 0.8489 - val_auc: 0.7424\n",
      "Epoch 5/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.3723 - binary_accuracy: 0.8501 - auc: 0.7360 - val_loss: 0.3814 - val_binary_accuracy: 0.8498 - val_auc: 0.7455\n",
      "Epoch 6/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.3678 - binary_accuracy: 0.8523 - auc: 0.7468 - val_loss: 0.3701 - val_binary_accuracy: 0.8518 - val_auc: 0.7484\n",
      "Epoch 7/25\n",
      "683/683 [==============================] - 14s 21ms/step - loss: 0.3645 - binary_accuracy: 0.8550 - auc: 0.7531 - val_loss: 0.3746 - val_binary_accuracy: 0.8487 - val_auc: 0.7499\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a4b938ce20>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimse = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.01, patience=3, verbose=1)\n",
    "model_3.compile(optimizer = optimse , loss='binary_crossentropy' , metrics= ['binary_accuracy' , tf.metrics.AUC(name='auc')])\n",
    "\n",
    "model_3.fit([X_train[:,:230],X_train[:,230:]],y_train,validation_data=([X_test[:,:230],X_test[:,230:]],y_test),\n",
    "            batch_size = 128, epochs = 25,callbacks = [early_stop ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\">Best Validation loss<br></th>\n",
    "    <th class=\"tg-0pky\">Best Train Loss</th>\n",
    "    <th class=\"tg-0pky\">Best Validation accuray</th>\n",
    "    <th class=\"tg-0pky\">Best Validation AUC<br></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Model 1</td>\n",
    "    <td class=\"tg-0pky\">0.3740</td>\n",
    "    <td class=\"tg-0pky\">0.3492</td>\n",
    "    <td class=\"tg-0pky\">0.8488</td>\n",
    "    <td class=\"tg-0pky\">0.7546</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Model 2</td>\n",
    "    <td class=\"tg-0pky\">0.3783</td>\n",
    "    <td class=\"tg-0pky\">0.3577</td>\n",
    "    <td class=\"tg-0pky\">0.8502</td>\n",
    "    <td class=\"tg-0pky\">0.7423</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Model 3</td>\n",
    "    <td class=\"tg-0pky\">0.3701</td>\n",
    "    <td class=\"tg-0pky\">0.3645</td>\n",
    "    <td class=\"tg-0pky\">0.8518</td>\n",
    "    <td class=\"tg-0pky\">0.7499</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 has best validation loss and best validation accuracy but Model 1 has best ACU of 0.7546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
