{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sNKZq4XrXQh"
   },
   "source": [
    "# <font color='red'><b>Bootstrap assignment</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAHap1Z3FZC-"
   },
   "source": [
    "<b>There will be some functions that start with the word \"grader\" ex: grader_sampples(), grader_30().. etc, you should not change those function definition.\n",
    "\n",
    "Every Grader function has to return True.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuxBq_bvrwh2"
   },
   "source": [
    "<font color='blue'> <b>Importing packages</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m6ag91ijrQOs"
   },
   "outputs": [],
   "source": [
    "import numpy as np # importing numpy for numerical computation\n",
    "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
    "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "CcHOsONTt1K_",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sisodiya.bhoomendra/venvs/test/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEa_HqRZloH4"
   },
   "source": [
    "## <font color='red'><b>Task 1</b></font>\n",
    "\n",
    "<font color='red'> <b>Step - 1</b></font>\n",
    "\n",
    "\n",
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes\n",
    "\n",
    "* <font color='red'><b> Note - While selecting the random 60% datapoints from the whole data, make sure that the selected datapoints are all exclusive, repetition is not allowed. </b></font>\n",
    "\n",
    "<font color='red'><b>Step - 2 </b></font>\n",
    "\n",
    "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>\n",
    "\n",
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$\n",
    "\n",
    "<font color='red'> <b>Step - 3 </b></font>\n",
    "\n",
    "*  <font color='blue'><b>Calculating the OOB score </b></font>\n",
    "\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
    "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dme-N6TUCrY"
   },
   "source": [
    "# <font color='red'><b>Task 2</b></font>\n",
    "\n",
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6UcH1x9Uwrj"
   },
   "source": [
    "# <font color='red'><b>Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOC_AgsLU7OH"
   },
   "source": [
    "*  <font color='blue'><b>Given a single query point predict the price of house.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYs5jSFdVILe"
   },
   "source": [
    "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "Predict the house price for this point as mentioned in the step 2 of Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6rShd89t552"
   },
   "source": [
    "## <font color='red'><b>A few key points</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgTUXTouHEd"
   },
   "source": [
    "* Remember that the datapoints used for calculating MSE score contain some datapoints that were initially used while training the base learners (the 60% sampling). This makes these datapoints partially seen (i.e. the datapoints used for calculating the MSE score are a mixture of seen and unseen data).\n",
    "Whereas, the datapoints used for calculating OOB score have only the unseen data. This makes these datapoints completely unseen and therefore appropriate for testing the model's performance on unseen data.\n",
    "\n",
    "* Given the information above, if your logic is correct, the calculated MSE score should be less than the OOB score.\n",
    "\n",
    "* The MSE score must lie between 0 and 10.\n",
    "* The OOB score must lie between 10 and 35.\n",
    "\n",
    "* The difference between the left nad right confidence-interval values must not be more than 10. Make sure this is true for both MSE and OOB confidence-interval values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2fHTdS_zpgG"
   },
   "source": [
    "# <font color='blue'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0yGBuryOwHz"
   },
   "source": [
    "<font color='blue'><b>Step - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJXX8vf3z073"
   },
   "source": [
    "*  <font color='blue'> <b>Creating samples</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSVaWG1F4uCZ",
    "tags": []
   },
   "source": [
    "<font color='Orange'><b>Algorithm</b></font>\n",
    "\n",
    "![alt text](https://i.imgur.com/OfcFrUP.jpg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_oWoN97BhDY"
   },
   "source": [
    "*  <font color='blue'><b> Write code for generating samples</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ph_6D2SDzz7F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generating_samples(input_data, target_data):\n",
    "\n",
    "    '''In this function, we will write code for generating 30 samples '''\n",
    "    # you can use random.choice to generate random indices without replacement\n",
    "    # Please have a look at this link https://docs.scipy.org/doc/numpy-1.16.0/reference/generated/numpy.random.choice.html for more details\n",
    "    # Please follow above pseudo code for generating samples \n",
    "    selecting_rows = np.random.choice(input_data.shape[0],303,replace = False)# No repetation \n",
    "    replacing_rows = np.random.choice(selecting_rows.shape[0],203,replace = True)# No repetation\n",
    "    selecting_col = np.random.choice(input_data.shape[1],np.random.randint(3,input_data.shape[1]),replace = False)\n",
    "    sample_data = input_data[selecting_rows,:][:,selecting_col]# This will have 303 rows \n",
    "    target_of_sample_data = target_data[selecting_rows]# 303 rows\n",
    "    # Replicate Data \n",
    "    replicated_sample_data = sample_data[replacing_rows]# selecting 203 rows\n",
    "    target_of_replicated_data = target_of_sample_data[replacing_rows]# 203 rows\n",
    "    #concatiting data\n",
    "    final_sample_data = np.vstack([sample_data,replicated_sample_data])\n",
    "    final_target_data = np.vstack([target_of_sample_data.reshape(-1,1),target_of_replicated_data.reshape(-1,1)])\n",
    "    # print(final_sample_data.shape, final_target_data.shape)\n",
    "    return final_sample_data,final_target_data,set(selecting_rows),set(selecting_col)# returning as set to make lookup faster\n",
    "    # return sampled_input_data , sampled_target_data,selected_rows,selected_columns\n",
    "    #note please return as lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MivEQFlm7iOg"
   },
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AVvuhNzm7uld",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)# This will only be correct if data points are unique\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    # print(len(set([str(i) for i in a])))# This should have been 303 \n",
    "    # print(len(a)-len(set([str(i) for i in a])))\n",
    "    #print(length,sampled,rows_length,column_length)\n",
    "    \n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "a,b,c,d = generating_samples(x, y)\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4LSsmn4Jn2_"
   },
   "source": [
    "*  <font color='blue'> <b>Create 30 samples </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ec7MN6sL2BZ",
    "tags": []
   },
   "source": [
    "![alt text](https://i.imgur.com/p8eZaWL.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XXlKWjCcBvTk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use generating_samples function to create 30 samples \n",
    "# store these created samples in a list\n",
    "def createSample(x,y,sample_count):\n",
    "    list_input_data =[]\n",
    "    list_output_data =[]\n",
    "    list_selected_row= []\n",
    "    list_selected_columns=[]\n",
    "    for i in range(sample_count):\n",
    "        a,b,c,d = generating_samples(input_data=x,target_data=y)\n",
    "        list_input_data.append(a)\n",
    "        list_output_data.append(b)\n",
    "        list_selected_row.append(c)\n",
    "        list_selected_columns.append(d)\n",
    "        # print(a.shape[1],len(d))\n",
    "    return list_input_data,list_output_data,list_selected_row,list_selected_columns\n",
    "list_input_data,list_output_data,list_selected_row,list_selected_columns = createSample(x,y,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXUz9VFiMQkh",
    "tags": []
   },
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hCvIq8NuMWOC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = createSample(x,y,30)\n",
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pv-mkZkO6dh"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whaHCPB0O8qF",
    "tags": []
   },
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBy4zXSWPtU8",
    "tags": []
   },
   "source": [
    "<font color='orange'><b>Flowchart for building tree</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xvH06HPQBdP",
    "tags": []
   },
   "source": [
    "![alt text](https://i.imgur.com/pcXfSmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRwPO_uHQjul",
    "tags": []
   },
   "source": [
    "*  <font color='blue'><b> Write code for building regression trees</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21j8BKfAQ1U8"
   },
   "source": [
    "<font color='orange'><b>Flowchart for calculating MSE </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q0mTBD2RBx_",
    "tags": []
   },
   "source": [
    "![alt text](https://i.imgur.com/sPEE618.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuclPDMnSz8F"
   },
   "source": [
    "<font color='blue'><b>Step - 3 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESb9FSIDTM5V"
   },
   "source": [
    "<font color='orange'><b>Flowchart for calculating OOB score</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB-d6NMETbd9",
    "tags": []
   },
   "source": [
    "![alt text](https://i.imgur.com/95S5Mtm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionRandomForest():# Can make it such that you can pass an array of models a general bagging frame_work \n",
    "\n",
    "    \n",
    "    def __init__(self,n_estimator,ratio_unique,min_col,verbose=True) -> None:\n",
    "        self.n_estimator  = n_estimator\n",
    "        self.ratio_unique = ratio_unique\n",
    "        self.min_col = min_col # for forming a bootstaped sample\n",
    "        self.input_row = None\n",
    "        self.input_cols = None\n",
    "        self.models = None\n",
    "        self.rows = None\n",
    "        self.cols = None\n",
    "        self.unique_pt = None\n",
    "        self.mse = None\n",
    "        self.oob_score = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        self.input_row = x.shape[0]\n",
    "        self.input_cols = x.shape[1]\n",
    "        self.unique_pt = int(self.input_row*self.ratio_unique)\n",
    "        #print(self.input_row,self.input_cols,self.unique_pt)\n",
    "        self.rows = [] # this will store the indexs of the samples that are bootstraped from the data\n",
    "        self.cols = [] # this will store the cols that are selected and both will be in the same order\n",
    "        self.models = []\n",
    "        total_pred = np.zeros(self.input_row)\n",
    "        total_oob = np.zeros(self.input_row)\n",
    "        count_oob_update = np.zeros(self.input_row)# Count maintains how many times a point has updated\n",
    "        for i in tqdm(range(self.n_estimator),disable = self.verbose) :\n",
    "            row,col = self.genrate_sample()\n",
    "            self.rows.append(row)\n",
    "            self.cols.append(col)\n",
    "            model = DecisionTreeRegressor()\n",
    "            model.fit(x[row,:][:,col],y[row])\n",
    "            total_pred +=  model.predict(x[:,col])\n",
    "            oob_idx  = [ i for i in range(self.input_row) if i not in row ]\n",
    "            total_oob[oob_idx] += model.predict(x[oob_idx,:][:,col])\n",
    "            count_oob_update[oob_idx] += 1 \n",
    "            self.models.append(model)\n",
    "        mean_oob_pred = total_oob/count_oob_update # To make is safe all the zeros should be replaced with 1's\n",
    "        mean_pred = total_pred/self.n_estimator\n",
    "        self.oob_score = mean_squared_error(mean_oob_pred,y)\n",
    "        self.mse = mean_squared_error(mean_pred,y)\n",
    "    \n",
    "    def precit(self,x):\n",
    "        total_pred = np.zeros(x.shape[0])# Number of outputs\n",
    "        for i in range(self.n_estimator) :\n",
    "            #pdb.set_trace()\n",
    "            total_pred += self.models[i].predict(x[:,self.cols[i]]) \n",
    "        return total_pred/self.n_estimator\n",
    "\n",
    "    def genrate_sample(self):\n",
    "        selecting_rows = np.random.choice(self.input_row, self.unique_pt ,replace = False)# No repetation \n",
    "        replacing_rows = np.random.choice(selecting_rows,self.input_row - self.unique_pt,replace = True)# This can be repeating\n",
    "        selecting_col = np.random.choice(self.input_cols,np.random.randint(self.min_col,self.input_cols),replace = False)\n",
    "        return np.hstack((selecting_rows,replacing_rows)) , selecting_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 191.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MSE IS :  2.4613961153472075\n",
      "THE oob_score IS :  12.767203173112293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RegressionRandomForest(n_estimator=30,ratio_unique=0.6,min_col=3,verbose=False)\n",
    "model.fit(x=x,y=y)\n",
    "print(\"THE MSE IS : \",model.mse)\n",
    "print(\"THE oob_score IS : \",model.oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbuiwX3OUjUI"
   },
   "source": [
    "# <font color='blue'><b>Task 2</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ceW5-D88Uswi"
   },
   "outputs": [],
   "source": [
    "def calculate_CI(itr = 35):\n",
    "    mses = []\n",
    "    oob_scores = []\n",
    "    for i in tqdm(range(itr)):\n",
    "        model_temp = RegressionRandomForest(n_estimator=30,ratio_unique=0.6,min_col=3)\n",
    "        model_temp.fit(x=x,y=y)\n",
    "        mses.append(model_temp.mse)\n",
    "        oob_scores.append(model_temp.oob_score)\n",
    "    mses = np.array(mses)\n",
    "    oob_scores = np.array(oob_scores)\n",
    "    mse_std_error = np.std(mses)/np.sqrt(itr)\n",
    "    oob_std_error = np.std(oob_scores)/np.sqrt(itr)\n",
    "    print(f\"\"\"The 95% Confidence interval of MSE is interval [{np.mean(mses) - 1.96*mse_std_error} , \n",
    "          {np.mean(mses) + 1.96*mse_std_error}] \"\"\")\n",
    "    print(f\"\"\"The 95% Confidence interval of OOB_score is interval [{np.mean(oob_scores) - 1.96*oob_std_error} , \n",
    "          {np.mean(oob_scores) + 1.96*oob_std_error}] \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:04<00:00,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% Confidence interval of MSE is interval [2.325741732488217 , \n",
      "          2.502196564950348] \n",
      "The 95% Confidence interval of OOB_score is interval [13.457747496125508 , \n",
      "          14.34274336744045] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_CI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKTnJdiBVS_e"
   },
   "source": [
    "# <font color='blue'><b>Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXxrvZqHV1Fr"
   },
   "source": [
    "<font color='orange'><b>Flowchart for Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyjwEJ62V6a6"
   },
   "source": [
    "<b>Hint: </b> We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\"  to 30 models and perform the regression on the output generated by 30 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0emSwLL7VurD"
   },
   "source": [
    "![alt text](https://i.imgur.com/Y5cNhQk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29hjwKlWWDfo"
   },
   "source": [
    "*  <font color='blue'><b> Write code for TASK 3 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DJHTGEZgWJjR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction:  [21.43666667]\n"
     ]
    }
   ],
   "source": [
    "xq = np.array([0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60])\n",
    "print(\"The prediction: \",model.precit(xq.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIcax45hWKT-"
   },
   "source": [
    "# OBSERVATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather then doing simple mean can we do weighted mean based on oob_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "\n",
    "In the step 1 of the task we did bootstraped sampling from the data set for ever sample we only took 303 data point(60 %) rest 203 are just random duplication from this 303 points this is done because in bootstraping we can have dupication from the sample.\n",
    "\n",
    "Bootstrapping is loosely based on the law of large numbers, which states that if you sample over and over again, your data should approximate the true population data. This works, perhaps surprisingly, even when you’re using a single sample to generate the data.\n",
    "\n",
    "    1. An empirical bootstrap sample is drawn from observations.\n",
    "    \n",
    "    2. A parametric bootstrap sample is drawn from a parameterized distribution (e.g. a normal distribution).\n",
    "So rather than using the given sample as a whole to train we are using bootstraped samples to trian multiple models in the hope that when we combines the models which are trained on all this different samples we would get better results as suggested by law of large numbers intuitively  \n",
    "\n",
    "\n",
    "In the second step we feed the models with all the x and then took the average of all the prediction from 30 different models and calculated its mean sqaured error with orginal data labels \n",
    "\n",
    "In the third step what happed was we were calculating the same mean squared error but for the oob points this are set of points which were not the part of the traning for a given model and we averages all the values of oob predction as well and then caculated the same mean squared error with oob predictions and labeled data.\n",
    "\n",
    "## TASK 2\n",
    "\n",
    "We are creating confidence interval for the MSE and OOB Scores by make 35 regression random forest models and colleting their MSE and OOB score to calculated which will gives us bound on how many CI will contain the mean.\n",
    "\n",
    "we are viewing CI in terms of fequentest way when we say If repeated random samples were taken and the 95% confidence interval was computed for each sample, 95% of the intervals would contain the population mean but if we go by the Baisian way then it should be that probablity of mean to be inside of CI is 95%?\n",
    "\n",
    "## TASK 3\n",
    "\n",
    "In the we are just given a test point for which we have to provide a regression value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
