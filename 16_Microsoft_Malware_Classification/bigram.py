import os
import numpy as np
from glob import glob

from scipy.sparse import lil_matrix
import pickle as pkl
from tqdm import tqdm

import  multiprocessing as mlt



all_hex = []
for i in range(256):
    if i<16:
        all_hex.append('0'+hex(i)[2:].upper())
    else:
        all_hex.append(hex(i)[2:].upper())
all_hex.append("??")
print(all_hex)
print(len(all_hex))
vocab_idx = dict()

for idx,hex in enumerate(all_hex):
    vocab_idx[hex] = idx
vocab_size = 257
for hex_code_1 in all_hex:
    for hex_code_2 in all_hex:
        vocab_idx[hex_code_1+hex_code_2] = vocab_size
        vocab_size+=1
print(vocab_idx)
print("Vocabulary size : ",vocab_size)
#removal of addres from byte files
# contents of .byte files
# ----------------
#00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 
#-------------------
#we remove the starting address 00401000

#Converting all the byte files to text files 
scratch_folder = "/scratch/sisodiya.bhoomeendra/"
data = 'byte_f/byte_files/'
path_ = os.path.join(scratch_folder,data)
print(path_)
resutls = 'results'

def putPickel(var,path):
    file = open(path,'wb')
    pkl.dump(var,file)
    file.close()

def getPickel(path):
    file = open(path,'rb')
    return pkl.load(file)
# as we will be doing multi-threading with 12 threads we can use shared memory approximate time 

def bfileToVector(file_path):
    
    file = open(file_path,'r')
    prv = None
    vec = np.zeros(vocab_size) # becomes the vector of that line
    
    for line in file:
        for nxt in line.split()[1:]:
            vec[vocab_idx[nxt]]+=1
            if prv is not None:
                vec[vocab_idx[prv+nxt]]+=1
            prv = nxt
        prv = None
    
    return vec


def process(file_lists,id):# that need to be processed
    # results = open(os.path.join(base_path,scratch_folder,resutls,"sampleResult.csv"),'w')# can use csr matrix directly
    total_byte_docs = len(file_lists)
    lil_vectors = lil_matrix((total_byte_docs,vocab_size),dtype=np.int32) # effect on size when left  
    pbar = tqdm(total=total_byte_docs,desc=str(id))
    for idx,file_path in enumerate(file_lists):
        vec = bfileToVector(file_path)
        lil_vectors[idx,:] = vec
        pbar.update(1)
    # Save csr matrix as pickel
    # csr_vectors = lil_vectors.tocsr()
    # print(lil_vectors.__len__)
    # putPickel(csr_vectors,os.path.join(base_path,scratch_folder,resutls,"csr_vec.pkl"))
    putPickel(lil_vectors,os.path.join(scratch_folder,resutls,"lil_vec"+str(id)+".pkl"))
    # results.close()

def extractPaths():
    total_byte_docs = 0
    file_index = dict()
    bfile_paths = []
    pbar = tqdm(total=10000,desc="File extractor")
    for file_path in glob(path_+"/*"):
        f_type = file_path.split('.')[-1]
        if f_type == 'bytes':
            if total_byte_docs%100:
                pbar.update(100)
            file_name = file_path.split('.')[1].split('/')[-1]
            file_index[file_name] = total_byte_docs
            total_byte_docs += 1
            bfile_paths.append(file_path)
    putPickel(bfile_paths,os.path.join(scratch_folder,resutls,"bfile_paths.pkl"))
    print("file are saved")

def runner():
    total_processors = 80
    # make 80 different list and 80 differen dicts
    bfile_path = getPickel(os.path.join(scratch_folder,resutls,"bfile_paths.pkl"))
    seg_size = len(bfile_path)//total_processors +1
    segs_paths = [[] for i in range(total_processors)]

    for idx, path  in enumerate(bfile_path):
        segs_paths[idx//seg_size].append(path)
    print([len(a) for a in segs_paths])

    pool = mlt.Pool(processes=total_processors)
    process_list = []
    for idx,segs in enumerate(segs_paths):
        pr = pool.apply_async(process,args=(segs,idx,))
        process_list.append(pr)

    for pr in process_list:
        pr.wait()

if __name__=="__main__":
    extractPaths()
    runner()