{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDMgSstPYv0P"
   },
   "source": [
    "# Text Classification:\n",
    "\n",
    "## Data\n",
    "<pre>\n",
    "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
    "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
    "so from document name, you can extract the label for that document.\n",
    "4. Now our problem is to classify all the documents into any one of the class.\n",
    "5. Below we provided count plot of all the labels in our data. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64U9NzWFYv0V",
    "outputId": "f3f19ed2-f637-4a8c-cff7-40a603025e96"
   },
   "outputs": [],
   "source": [
    "### count plot of all the class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mK4TJOFYv0h"
   },
   "source": [
    "## Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlqYFVI3Yv0k"
   },
   "source": [
    "#### sample document\n",
    "<pre>\n",
    "<font color='blue'>\n",
    "Subject: A word of advice\n",
    "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
    "\n",
    "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
    ">\n",
    ">I've said 100 times that there is no \"alternative\" that should think you\n",
    ">might have caught on by now.  And there is no \"alternative\", but the point\n",
    ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
    ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
    ">solve them.\n",
    "\n",
    "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
    "those who are doing it.\n",
    "\n",
    "Jim\n",
    "--\n",
    "Have you washed your brain today?\n",
    "</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAR5HoR1Yv0m"
   },
   "source": [
    "### Preprocessing:\n",
    "<pre>\n",
    "useful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n",
    "\n",
    "<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \n",
    "after that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \n",
    "In one doc, if we have 2 or more mails, get all.\n",
    "<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \n",
    "append all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \n",
    "Some sample output was shown below. \n",
    "\n",
    "> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n",
    "\n",
    "preprocessing:\n",
    "[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n",
    "[nyx edu mimsy umd edu umd edu]\n",
    "\n",
    "<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KavKDD9FYv0p",
    "outputId": "0b87ab7b-46df-4995-eaca-4f5831ad223e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['juliet caltech edu',\n",
       "       'coding bchs edu newsgate sps mot austlcm sps mot austlcm sps mot com  dna bchs edu',\n",
       "       'batman bmd trw', ..., 'rbdc wsnc org dscomsa desy zeus  desy',\n",
       "       'rbdc wsnc org morrow stanford edu pangea Stanford EDU',\n",
       "       'rbdc wsnc org apollo apollo'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have collected all emails and preprocessed them, this is sample output\n",
    "preprocessed_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obReqs55Yv0v",
    "outputId": "10770414-9be0-4d63-9587-5363a8c10c4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18828"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIovFDQzYv03"
   },
   "source": [
    "<pre>\n",
    "<font color='blue'><b>3.</b></font> Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove \n",
    "the word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n",
    "<b>Eg: if we have sentance like \"Subject: Re: Gospel Dating @ \\r\\r\\n\" --> You have to get \"Gospel Dating\"</b> \n",
    "Save all this data into another list/array. \n",
    "\n",
    "<font color='blue'><b>4.</b></font> After you store it in the list, Replace those sentances in original text by space.\n",
    "\n",
    "<font color='blue'><b>5.</b></font> Delete all the sentances where sentence starts with <b>\"Write to:\"</b> or <b>\"From:\"</b>.\n",
    "> In the above sample document check the 2nd line, we should remove that\n",
    "\n",
    "<font color='blue'><b>6.</b></font> Delete all the tags like \"< anyword >\"\n",
    "> In the above sample document check the 4nd line, we should remove that \"< 65882@mimsy.umd.edu >\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>7.</b></font> Delete all the data which are present in the brackets. \n",
    "In many text data, we observed that, they maintained the explanation of sentence \n",
    "or translation of sentence to another language in brackets so remove all those.\n",
    "<b>Eg: \"AAIC-The course that gets you HIRED(AAIC - Der Kurs, der Sie anstellt)\" --> \"AAIC-The course that gets you HIRED\"</b>\n",
    "\n",
    "> In the above sample document check the 4nd line, we should remove that \"(Charley Wingate)\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>8.</b></font> Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n",
    "\n",
    "<font color='blue'><b>9.</b></font> Remove all the words which ends with <b>\":\"</b>.\n",
    "<b>Eg: \"Anyword:\"</b>\n",
    "> In the above sample document check the 4nd line, we should remove that \"writes:\"\n",
    "\n",
    "\n",
    "<font color='blue'><b>10.</b></font> Decontractions, replace words like below to full words. \n",
    "please check the donors choose preprocessing for this \n",
    "<b>Eg: can't -> can not, 's -> is, i've -> i have, i'm -> i am, you're -> you are, i'll --> i will </b>\n",
    "\n",
    "<b> There is no order to do point 6 to 10. but you have to get final output correctly</b>\n",
    "\n",
    "<font color='blue'><b>11.</b></font> Do chunking on the text you have after above preprocessing. \n",
    "Text chunking, also referred to as shallow parsing, is a task that \n",
    "follows Part-Of-Speech Tagging and that adds more structure to the sentence.\n",
    "So it combines the some phrases, named entities into single word.\n",
    "So after that combine all those phrases/named entities by separating <b>\"_\"</b>. \n",
    "And remove the phrases/named entities if that is a \"Person\". \n",
    "You can use <b>nltk.ne_chunk</b> to get these. \n",
    "Below we have given one example. please go through it. \n",
    "\n",
    "useful links: \n",
    "<a href='https://www.nltk.org/book/ch07.html'>https://www.nltk.org/book/ch07.html</a>\n",
    "<a href='https://stackoverflow.com/a/31837224/4084039'>https://stackoverflow.com/a/31837224/4084039</a>\n",
    "<a href='http://www.nltk.org/howto/tree.html'>http://www.nltk.org/howto/tree.html</a>\n",
    "<a href='https://stackoverflow.com/a/44294377/4084039'>https://stackoverflow.com/a/44294377/4084039</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lAaKQ6EYv04",
    "outputId": "53b66a94-acef-4002-e51c-002bde4178b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am living in the New York --> [('i', 'NN'), ('am', 'VBP'), ('living', 'VBG'), ('in', 'IN'), ('the', 'DT'), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])]\n",
      " \n",
      "--------------------------------------------------\n",
      " \n",
      "My name is Srikanth Varma --> [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('PERSON', [('Srikanth', 'NNP'), ('Varma', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "#i am living in the New York\n",
    "print(\"i am living in the New York -->\", list(chunks))\n",
    "print(\" \")\n",
    "print(\"-\"*50)\n",
    "print(\" \")\n",
    "#My name is Srikanth Varma\n",
    "print(\"My name is Srikanth Varma -->\", list(chunks1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XV8gzLUjYv0-"
   },
   "source": [
    "<pre>We did chunking for above two lines and then We got one list where each word is mapped to a \n",
    "POS(parts of speech) and also if you see \"New York\" and \"Srikanth Varma\", \n",
    "they got combined and represented as a tree and \"New York\" was referred as \"GPE\" and \"Srikanth Varma\" was referred as \"PERSON\". \n",
    "so now you have to Combine the \"New York\" with <b>\"_\"</b> i.e \"New_York\"\n",
    "and remove the \"Srikanth Varma\" from the above sentence because it is a person.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpaC-KF3Yv1A"
   },
   "source": [
    "<pre>\n",
    "<font color='blue'><b>13.</b></font> Replace all the digits with space i.e delete all the digits. \n",
    "> In the above sample document, the 6th line have digit 100, so we have to remove that.\n",
    "\n",
    "<font color='blue'><b>14.</b></font> After doing above points, we observed there might be few word's like\n",
    " <b> \"_word_\" (i.e starting and ending with the _), \"_word\" (i.e starting with the _),\n",
    "  \"word_\" (i.e ending with the _)</b> remove the <b>_</b> from these type of words. \n",
    "\n",
    "<font color='blue'><b>15.</b></font>  We also observed some words like <b> \"OneLetter_word\"- eg: d_berlin, \n",
    "\"TwoLetters_word\" - eg: dr_berlin </b>, in these words we remove the \"OneLetter_\" (d_berlin ==> berlin) and \n",
    "\"TwoLetters_\" (de_berlin ==> berlin). i.e remove the words \n",
    "which are length less than or equal to 2 after spliiting those words by \"_\". \n",
    "\n",
    "<font color='blue'><b>16.</b></font> Convert all the words into lower case and lowe case \n",
    "and remove the words which are greater than or equal to 15 or less than or equal to 2.\n",
    "\n",
    "<font color='blue'><b>17.</b></font> replace all the words except \"A-Za-z_\" with space. \n",
    "\n",
    "<font color='blue'><b>18.</b></font> Now You got Preprocessed Text, email, subject. create a dataframe with those. \n",
    "Below are the columns of the df. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hB43OGEfYv1C",
    "outputId": "945bc8a4-1f99-4410-94c8-c776a405b5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'class', 'preprocessed_text', 'preprocessed_subject',\n",
      "       'preprocessed_emails'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM6A19xFYv1I",
    "outputId": "9de13fa8-6604-49a2-8013-6b22f0a256a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                    From: arc1@ukc.ac.uk (Tony Curtis)\\r\\r\\r\\nSubj...\n",
      "class                                                         alt.atheism\n",
      "preprocessed_text       said re is article if followed the quoting rig...\n",
      "preprocessed_subject                                christian morality is\n",
      "preprocessed_emails                                   ukc mac macalstr edu\n",
      "Name: 567, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data.iloc[400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfWUeIN1Yv1N"
   },
   "source": [
    "### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "email_rx = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "tag_re = r'<\\s*.*?\\s*>'\n",
    "bracket_re = r'\\(\\s*.*?\\s*\\)'\n",
    "colan_rx = r'\\w*\\s*:'\n",
    "underscore_re = r'_+\\w*|\\w*_'\n",
    "\n",
    "def is_email(email):\n",
    "    if(re.fullmatch(email_rx, email)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEGEHTNQYv1N"
   },
   "outputs": [],
   "source": [
    "# Point 1\n",
    "def clean_email(email_list):\n",
    "    final_output = ''\n",
    "    for email in email_list:\n",
    "        process_email = [ txt for txt in email.split(\"@\")[1].split('.') if len(txt) >2 and txt!='com']\n",
    "        for token in process_email:\n",
    "            final_output += token + ' '\n",
    "        # print(email,process_email,final_output)\n",
    "    return final_output\n",
    "# Point 2\n",
    "def clean_subject(line):\n",
    "    subject = re.sub(colan_rx,'',line)\n",
    "    subject = re.sub(pattern='\\n',repl=' ',string=subject)\n",
    "    subject = re.sub(pattern='\\t',repl=' ',string=subject)\n",
    "    subject = re.sub(pattern=r'[^\\w+|\\s*]',repl = '', string =subject)\n",
    "    # print(line,\"\\n\",subject)\n",
    "    return subject\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "#Point 11 12\n",
    "def remove_person_and_comdine(text):\n",
    "    # tokenize then do pos then pass it to ne_chuking\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    id_txt = nltk.chunk.ne_chunk(pos_tagged)\n",
    "    output = ''\n",
    "    for elm in list(id_txt):\n",
    "        if type(elm) is tuple:\n",
    "            if elm[1] != 'PERSON':\n",
    "                output += elm[0] + ' '\n",
    "        else:\n",
    "            if elm.label() != 'PERSON':\n",
    "                txt = elm.leaves()\n",
    "                processed = ''\n",
    "                for word in txt:\n",
    "                    processed += word[0] + '_'\n",
    "                if processed[-1] =='_':\n",
    "                    output += processed[:-1]\n",
    "    return output\n",
    "\n",
    "\n",
    "def preprocess(Input_Text):# The input is a list of lines on text \n",
    "    \"\"\"Do all the Preprocessing as shown above and\n",
    "    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n",
    "\n",
    "    email_list  = re.findall(email_rx,Input_Text)# Point 1\n",
    "    list_of_preproessed_emails = clean_email(email_list)# Point 1\n",
    "    \n",
    "    Input_Text = re.sub(email_rx,\" \",Input_Text)# Point 2\n",
    "\n",
    "    # print(\"Emails Removed : \\n\",Input_Text)\n",
    "    \n",
    "    p_text = ''# Processed text\n",
    "    subject = ''\n",
    "    for line in Input_Text.split('\\n'):\n",
    "        if line.find(\"Subject:\") != -1:\n",
    "            subject += clean_subject(line)# Point 3\n",
    "            p_text+= ' '# Point 4\n",
    "        else:\n",
    "            if line.find(\"Write to:\") == -1 and line.find(\"From:\") == -1:# Point 5\n",
    "                p_text += line + ' '\n",
    "    # print(\"Subject write and from removed \\n: \",p_text)\n",
    "\n",
    "    p_text = re.sub(pattern=tag_re,repl=' ',string=p_text)# Point 6\n",
    "    p_text = re.sub(pattern=bracket_re,repl=' ',string=p_text)# Point 7\n",
    "    p_text = re.sub(pattern='\\n',repl=' ',string=p_text)# Point 8\n",
    "    p_text = re.sub(pattern='\\t',repl=' ',string=p_text)# Point 8\n",
    "    p_text = p_text.replace('\\\\',' ')# Point 8\n",
    "    # print(\"Tags brackets \\\\n \\\\t \\\\ is removed : \",p_text)\n",
    "    p_text = re.sub(r'\\s+',' ',p_text)\n",
    "    # print(\"Important: \\n\",p_text.split(' '))\n",
    "    final_text = ''\n",
    "    for word in p_text.split(' '):# Point 9\n",
    "        \n",
    "        if len(word) > 0 and word[-1] != ':':\n",
    "            final_text+= word + ' '\n",
    "    # print(\"Words ending with colan removed : \\n\",final_text)\n",
    "    final_text = decontracted(final_text)# Point 10\n",
    "    # print(\"Words decontracted :\\n\",final_text)\n",
    "    # Chunking\n",
    "    final_text = re.sub(r'[^\\w+|\\s*]',' ',final_text)\n",
    "    \n",
    "    final_text = remove_person_and_comdine(final_text) \n",
    "    # print(\"Chucking step is done : \\n\",final_text)    \n",
    "    # Point 11,12\n",
    "    final_text = re.sub(pattern='\\d+',repl=' ',string=final_text)# Point 13\n",
    "    final_text = re.sub(pattern=underscore_re,repl='',string=final_text)# Point 14\n",
    "    #Point 15\n",
    "\n",
    "    final_text = final_text.lower()# Point 16\n",
    "    \n",
    "    # print(\"Impofinal_text\\n\",final_text)\n",
    "    \n",
    "    p_text = ''\n",
    "    for word in final_text.split(\" \"):\n",
    "        if len(word)>14 or len(word)<3:\n",
    "            continue\n",
    "        else:\n",
    "            if re.fullmatch('[a-zA-Z_]*',word) is not None:\n",
    "                p_text += word + ' ' \n",
    "    # print(\"Final text :\\n\",p_text)   \n",
    "    # print(list_of_preproessed_emails,subject,p_text)\n",
    "    return (list_of_preproessed_emails,subject,p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bhoomeendra/Applied ai/Assignment/20_CNN_with_textdata/documents/alt.atheism_49960.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/bhoomeendra/Applied ai/Assignment/20_CNN_with_textdata/documents/alt.atheism_49960.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_text)\n\u001b[1;32m      4\u001b[0m preprocess(input_text)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bhoomeendra/Applied ai/Assignment/20_CNN_with_textdata/documents/alt.atheism_49960.txt'"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/bhoomeendra/Applied ai/Assignment/20_CNN_with_textdata/documents/alt.atheism_49960.txt\"\n",
    "input_text = open(file_path,'r').read()\n",
    "print(input_text)\n",
    "preprocess(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home2/sisodiya.bhoomendra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home2/sisodiya.bhoomendra/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home2/sisodiya.bhoomendra/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i am living in the New_York'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "test = \"i am living in the New York \"\n",
    "remove_person_and_comdine(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home2/sisodiya.bhoomendra/Applied_ai/21_CNN_with_textdata'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18828/18828 [00:16<00:00, 1116.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the number of file which i am not able to open  72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm \n",
    "doc_path = \"/home/bhoomeendra/Applied ai/Assignment/20_CNN_with_textdata/documents/*\"\n",
    "ada_path = '/home2/sisodiya.bhoomendra/Applied_ai/21_CNN_with_textdata/documents/*'\n",
    "processed = []\n",
    "file_paths = []\n",
    "count = 0\n",
    "label = []\n",
    "    \n",
    "for file_path in tqdm(glob(ada_path)):\n",
    "    file_paths.append(file_path.split('/')[-1])\n",
    "    # print(file_path)\n",
    "    file = open(file_path,'r')\n",
    "    try:\n",
    "        input_text = file.read()\n",
    "        label.append(file_path.split('/')[-1].split('_')[0])\n",
    "        #processed.append(preprocess(input_text))\n",
    "        file.close()\n",
    "    except BaseException as e:\n",
    "        count+=1\n",
    "        file.close()\n",
    "        continue\n",
    "print(\"These are the number of file which i am not able to open \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df  = pd.DataFrame(processed)\n",
    "df.head()\n",
    "df.to_csv(\"preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table = pd.read_csv('preprocessed.csv')\n",
    "processed = table.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceASjKizYv1U"
   },
   "source": [
    "### Code checking:\n",
    "\n",
    "<font color='red' size=4>\n",
    "After Writing preprocess function. call that functoin with the input text of 'alt.atheism_49960' doc and print the output of the preprocess function\n",
    "<br>\n",
    "This will help us to evaluate faster, based on the output we can suggest you if there are any changes.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2x3og_iaYv1S"
   },
   "source": [
    "### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3ucJLtWYv1V"
   },
   "source": [
    "### Training The models to Classify: \n",
    "\n",
    "<pre>\n",
    "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n",
    "\n",
    "2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n",
    "\n",
    "3. Analyze your text data and pad the sequnce if required. \n",
    "Sequnce length is not restricted, you can use anything of your choice. \n",
    "you need to give the reasoning\n",
    "\n",
    "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
    "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n",
    "\n",
    "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
    "and try to optimize that models.  \n",
    "\n",
    "6. For every model use predefined Glove vectors. \n",
    "<b>Don't train any word vectors while Training the model.</b>\n",
    "\n",
    "7. Use \"categorical_crossentropy\" as Loss. \n",
    "\n",
    "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
    "\n",
    "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
    "\n",
    "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
    "\n",
    "11. You are free to choose any Activation function, learning rate, optimizer.\n",
    "But have to use the same architecture which we are giving below.\n",
    "\n",
    "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
    "\n",
    "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
    "\n",
    "14. For Every model save your model to image ( Plot the model) with shapes \n",
    "and inlcude those images in the notebook markdown cell, \n",
    "upload those imgages to Classroom. You can use \"plot_model\" \n",
    "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18756/18756 [00:01<00:00, 12667.63it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = label\n",
    "thresh = 400\n",
    "for x in tqdm(processed):\n",
    "    text = ''\n",
    "    if type(x[1]) is str:\n",
    "        text += x[1]\n",
    "    if type(x[2]) is str:\n",
    "        text += x[2]\n",
    "    if type(x[3]) is str:\n",
    "        text += x[3]\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    # print(text)\n",
    "    if len(text.split(' ')) > thresh:\n",
    "        final = ''\n",
    "        count = 0 \n",
    "        for word in text.split(' '):\n",
    "            final += word+' '\n",
    "            count += 1\n",
    "            if count>= thresh:\n",
    "                break\n",
    "        X.append(final.strip())\n",
    "    else:\n",
    "        X.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(p.split(' ')) for p in X]\n",
    "min(lengths),max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.percentile(lengths,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGfCAYAAAC+8c0rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzlUlEQVR4nO3dfXRU1b3/8c9MHgbkhpEEyTAQILapFYPUGyxCW0EegtRIXWipxVLaUosVgilQWy61pq5bUukVaBMfipcrVErTdVW89l6LBB+iLEQhSAVKra6mGCAxto2TgDEJc/bvD36cdsjDTEKSeTjv11pnLeac7xn27qnJh332OdtljDECAABwMHe0GwAAABBtBCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4ydH8y19++WX99Kc/VVVVlWpra7V9+3bddNNNHdYuXrxYGzdu1Pr161VUVGTvb2lp0cqVK/XrX/9azc3Nmj59uh566CGNHDnSrmloaNCyZcv0zDPPSJLmzJmj0tJSXXzxxRG31bIsnTx5UmlpaXK5XD3pLgAA6GfGGDU1Ncnv98vt7nwcKKqB6PTp0xo/fry+/vWv6+abb+607umnn9Zrr70mv9/f7lhRUZF++9vfqry8XBkZGVqxYoUKCgpUVVWlpKQkSdL8+fN1/Phx7dixQ5L0rW99SwsWLNBvf/vbiNt68uRJZWVldbOHAAAgFtTU1IQMlpwvqoFo9uzZmj17dpc1J06c0NKlS/Xcc8/phhtuCDkWCAS0adMmPf7445oxY4YkaevWrcrKytKuXbs0a9YsHT16VDt27NDevXs1ceJESdKjjz6qSZMm6a233tJll10WUVvT0tIknf0fdPDgwd3tKgAAiILGxkZlZWXZv8c7E9VAFI5lWVqwYIG++93v6oorrmh3vKqqSm1tbcrPz7f3+f1+5ebmas+ePZo1a5ZeffVVeb1eOwxJ0jXXXCOv16s9e/Z0GohaWlrU0tJif25qapIkDR48mEAEAECcCTfdJaYnVd9///1KTk7WsmXLOjxeV1en1NRUDRkyJGR/Zmam6urq7Jphw4a1O3fYsGF2TUdKSkrk9XrtjdtlAAAkrpgNRFVVVfrZz36mzZs3d3sSszEm5JyOzj+/5nyrVq1SIBCwt5qamm61AQAAxI+YDUSvvPKK6uvrNWrUKCUnJys5OVnHjh3TihUrNGbMGEmSz+dTa2urGhoaQs6tr69XZmamXfPee++1+/7333/frumIx+Oxb49xmwwAgMQWs4FowYIFevPNN3Xw4EF78/v9+u53v6vnnntOkpSXl6eUlBRVVFTY59XW1urw4cOaPHmyJGnSpEkKBAJ6/fXX7ZrXXntNgUDArgEAAM4W1UnVp06d0jvvvGN/rq6u1sGDB5Wenq5Ro0YpIyMjpD4lJUU+n8+eCO31erVo0SKtWLFCGRkZSk9P18qVKzVu3Dj7qbPLL79c119/vW6//Xb94he/kHT2sfuCgoKInzADAACJLaqBaP/+/bruuuvsz8uXL5ckLVy4UJs3b47oO9avX6/k5GTNmzfPfjHj5s2b7XcQSdKvfvUrLVu2zH4abc6cOSorK+u9jgAAgLjmMsaYaDciHjQ2Nsrr9SoQCDCfCACAOBHp7++YnUMEAADQXwhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8WJ6tXsAABDfLMtSJG/4cblccrujN07DCBEAAOgTlmVpRNZoe03SrrYRWaNlWVbU2soIEQAA6BPGGNWdPK5bHqyUq4vRH2NZemLJlIhGkvoKgQgAAPQpl9sttzup0+PRGxf6B26ZAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAx4tqIHr55Zd14403yu/3y+Vy6emnn7aPtbW16Xvf+57GjRunQYMGye/366tf/apOnjwZ8h0tLS0qLCzU0KFDNWjQIM2ZM0fHjx8PqWloaNCCBQvk9Xrl9Xq1YMECffDBB/3QQwAAEA+iGohOnz6t8ePHq6ysrN2xDz/8UAcOHNA999yjAwcO6KmnntKf/vQnzZkzJ6SuqKhI27dvV3l5uXbv3q1Tp06poKBAwWDQrpk/f74OHjyoHTt2aMeOHTp48KAWLFjQ5/0DAADxwWWMMdFuhCS5XC5t375dN910U6c1+/bt06c//WkdO3ZMo0aNUiAQ0CWXXKLHH39cX/rSlyRJJ0+eVFZWlp599lnNmjVLR48e1dixY7V3715NnDhRkrR3715NmjRJf/zjH3XZZZdF1L7GxkZ5vV4FAgENHjz4gvsLAECiCwaDSk5O1hcffkVud1KndZYV1H9/+3M6c+aMkpI6r+uJSH9/x9UcokAgIJfLpYsvvliSVFVVpba2NuXn59s1fr9fubm52rNnjyTp1VdfldfrtcOQJF1zzTXyer12TUdaWlrU2NgYsgEAgMQUN4Hoo48+0ve//33Nnz/fTnh1dXVKTU3VkCFDQmozMzNVV1dn1wwbNqzd9w0bNsyu6UhJSYk958jr9SorK6sXewMAAGJJXASitrY23XrrrbIsSw899FDYemOMXC6X/fmf/9xZzflWrVqlQCBgbzU1NT1rPAAAiHkxH4ja2to0b948VVdXq6KiIuT+n8/nU2trqxoaGkLOqa+vV2Zmpl3z3nvvtfve999/367piMfj0eDBg0M2AACQmGI6EJ0LQ2+//bZ27dqljIyMkON5eXlKSUlRRUWFva+2tlaHDx/W5MmTJUmTJk1SIBDQ66+/bte89tprCgQCdg0AAHC25Gj+5adOndI777xjf66urtbBgweVnp4uv9+vW265RQcOHND//u//KhgM2nN+0tPTlZqaKq/Xq0WLFmnFihXKyMhQenq6Vq5cqXHjxmnGjBmSpMsvv1zXX3+9br/9dv3iF7+QJH3rW99SQUFBxE+YAQCAxBbVQLR//35dd9119ufly5dLkhYuXKji4mI988wzkqRPfepTIee9+OKLmjp1qiRp/fr1Sk5O1rx589Tc3Kzp06dr8+bNIY/t/epXv9KyZcvsp9HmzJnT4buPAACAM8XMe4hiHe8hAgCge3gPEQAAQBwhEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMdLjnYDgHAsy5IxJmydy+WS203GBwB0H789ENMsy9KIrNFKTk4Ou43IGi3LsqLdZABAHGKECDHNGKO6k8d1y4OVcnUx+mMsS08smRLRSBIAAOcjECEuuNxuud1JnR5nXAgAcCG4ZQYAAByPESL0OiZBAwDiDb+N0KuYBA0AiEeMEKFXMQkaABCPCEToE5FOgg4Gg11+T7jjAAD0BgIRosJYluROksfjifCEvm0PAMDZCESIEiNZQd1cVil3Uue31qwzbXqycBp5CADQpwhEiKpwt9aMm1tmAIC+x1NmAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8QhEAADA8aIaiF5++WXdeOON8vv9crlcevrpp0OOG2NUXFwsv9+vgQMHaurUqTpy5EhITUtLiwoLCzV06FANGjRIc+bM0fHjx0NqGhoatGDBAnm9Xnm9Xi1YsEAffPBBH/cOAADEi6gGotOnT2v8+PEqKyvr8PjatWu1bt06lZWVad++ffL5fJo5c6aamprsmqKiIm3fvl3l5eXavXu3Tp06pYKCAgWDQbtm/vz5OnjwoHbs2KEdO3bo4MGDWrBgQZ/3DwAAxIfkaP7ls2fP1uzZszs8ZozRhg0btHr1as2dO1eStGXLFmVmZmrbtm1avHixAoGANm3apMcff1wzZsyQJG3dulVZWVnatWuXZs2apaNHj2rHjh3au3evJk6cKEl69NFHNWnSJL311lu67LLLOvz7W1pa1NLSYn9ubGzsza4DAIAYErNziKqrq1VXV6f8/Hx7n8fj0ZQpU7Rnzx5JUlVVldra2kJq/H6/cnNz7ZpXX31VXq/XDkOSdM0118jr9do1HSkpKbFvsXm9XmVlZfV2F+OKZVkKBoMRbQAAxJuYDUR1dXWSpMzMzJD9mZmZ9rG6ujqlpqZqyJAhXdYMGzas3fcPGzbMrunIqlWrFAgE7K2mpuaC+hPPLMvSiKzRSk5ODrt5PJ6zJ5nothkAgO6I6i2zSLhcrpDPxph2+853fk1H9eG+x+Px/OOXu8MZY1R38rhuebBSLnfXGdo606YnC6eRhwAAcSVmR4h8Pp8ktRvFqa+vt0eNfD6fWltb1dDQ0GXNe++91+7733///XajT+iay+2W253U5RYuMAEAEIti9rdXdna2fD6fKioq7H2tra2qrKzU5MmTJUl5eXlKSUkJqamtrdXhw4ftmkmTJikQCOj111+3a1577TUFAgG7BgAAOFtUb5mdOnVK77zzjv25urpaBw8eVHp6ukaNGqWioiKtWbNGOTk5ysnJ0Zo1a3TRRRdp/vz5kiSv16tFixZpxYoVysjIUHp6ulauXKlx48bZT51dfvnluv7663X77bfrF7/4hSTpW9/6lgoKCjp9wgzxK9JJ3S6XS25GswAA/19UA9H+/ft13XXX2Z+XL18uSVq4cKE2b96su+++W83NzbrzzjvV0NCgiRMnaufOnUpLS7PPWb9+vZKTkzVv3jw1Nzdr+vTp2rx5s5KSkuyaX/3qV1q2bJn9NNqcOXM6ffcR4pOxLMmdFPG8L59/pE7UHCMUAQAkRTkQTZ06VcZ0Pv3W5XKpuLhYxcXFndYMGDBApaWlKi0t7bQmPT1dW7duvZCmIuYZyQrq5rJKuZO6DjnGsvTEkild/n8PAOAsMf+UGdAd5yZ+d8Xqp7YAAOIH9wsAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjEYgAAIDjsXSHw1mWFXZNr0hXkAcAIF4RiBzMsiyNyBqtupPHIzuBtVABAAmKQORgxhjVnTyuWx6slMvd+d1T60ybniycRh4CACQsAhHCrhBv3Il5yyySW4Eul0vuLsIiACAxEIjgOMayJHeSPB5P2Fqff6RO1BwjFAFAgiMQwYGMZAV1c1ml3EmdBx1jWXpiyZSwk84BAPGPQATHCner0OrHtgAAoov7AAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPEIRAAAwPFY7R4IIxgMhq1xuVxyu/n3BQDEKwIR0AljWZI7SR6PJ2ytzz9SJ2qOEYoAIE4RiIBOGckK6uaySrmTOg86xrL0xJIpMsb0Y9sAAL2JQASE4XK75XYndXrc6se2AAD6BuP7AADA8QhEAADA8bhlBvQSnkYDgPhFIAIuEE+jAUD8IxABF4yn0QAg3hGIgF7C02gAEL9ietz+zJkz+sEPfqDs7GwNHDhQl156qe677z5Z1j9+tRhjVFxcLL/fr4EDB2rq1Kk6cuRIyPe0tLSosLBQQ4cO1aBBgzRnzhwdP368v7sDAABiVEwHovvvv1+PPPKIysrKdPToUa1du1Y//elPVVpaatesXbtW69atU1lZmfbt2yefz6eZM2eqqanJrikqKtL27dtVXl6u3bt369SpUyooKIhoEiwAAEh8MX3L7NVXX9UXvvAF3XDDDZKkMWPG6Ne//rX2798v6ezo0IYNG7R69WrNnTtXkrRlyxZlZmZq27ZtWrx4sQKBgDZt2qTHH39cM2bMkCRt3bpVWVlZ2rVrl2bNmhWdzgEAgJgR0yNEn/3sZ/X888/rT3/6kyTp97//vXbv3q3Pf/7zkqTq6mrV1dUpPz/fPsfj8WjKlCnas2ePJKmqqkptbW0hNX6/X7m5uXZNR1paWtTY2BiyAQCAxBTTI0Tf+973FAgE9MlPflJJSUkKBoP68Y9/rC9/+cuSpLq6OklSZmZmyHmZmZk6duyYXZOamqohQ4a0qzl3fkdKSkr0ox/9qDe7AwAAYlRMjxD95je/0datW7Vt2zYdOHBAW7Zs0X/8x39oy5YtIXUulyvkszGm3b7zhatZtWqVAoGAvdXU1PS8IwAAIKbF9AjRd7/7XX3/+9/XrbfeKkkaN26cjh07ppKSEi1cuFA+n0/S2VGg4cOH2+fV19fbo0Y+n0+tra1qaGgIGSWqr6/X5MmTO/27PR5PRC/aAwAA8S+mR4g+/PDDdm/0TUpKsh+7z87Ols/nU0VFhX28tbVVlZWVdtjJy8tTSkpKSE1tba0OHz7cZSACAADOEdMjRDfeeKN+/OMfa9SoUbriiiv0xhtvaN26dfrGN74h6eytsqKiIq1Zs0Y5OTnKycnRmjVrdNFFF2n+/PmSJK/Xq0WLFmnFihXKyMhQenq6Vq5cqXHjxtlPnQEAAGeL6UBUWlqqe+65R3feeafq6+vl9/u1ePFi/fCHP7Rr7r77bjU3N+vOO+9UQ0ODJk6cqJ07dyotLc2uWb9+vZKTkzVv3jw1Nzdr+vTp2rx5s5KSOn+rMAAAcI6YDkRpaWnasGGDNmzY0GmNy+VScXGxiouLO60ZMGCASktLQ17oCAAAcE6P5hBdeuml+tvf/tZu/wcffKBLL730ghsFAADQn3oUiP7yl790uOxFS0uLTpw4ccGNAgAA6E/dumX2zDPP2H9+7rnn5PV67c/BYFDPP/+8xowZ02uNAxJRpGvouVyudk9ZAgD6RrcC0U033STp7A/qhQsXhhxLSUnRmDFj9MADD/Ra44BEYixLcidF/H4rn3+kTtQcIxQBQD/oViD65/f/7Nu3T0OHDu2TRgGJyUhWUDeXVcqd1HXIMZalJ5ZMkTGmn9oGAM7Wo6fMqqure7sdgGO43G653V2/8sHqp7YAAM7q8WP3zz//vJ5//nnV19fbI0fn/Nd//dcFNwwAAKC/9CgQ/ehHP9J9992nCRMmaPjw4WEXUgUAAIhlPQpEjzzyiDZv3qwFCxb0dnsAAAD6XY8eX2ltbWVhVAAAkDB6FIi++c1vatu2bb3dFgAAgKjo0S2zjz76SBs3btSuXbt05ZVXKiUlJeT4unXreqVxAAAA/aFHgejNN9/Upz71KUnS4cOHQ44xwRoAAMSbHgWiF198sbfbAQAAEDWsCQAAAByvRyNE1113XZe3xl544YUeNwgAAKC/9SgQnZs/dE5bW5sOHjyow4cPt1v0FQAAINb1KBCtX7++w/3FxcU6derUBTUIAACgv/XqHKKvfOUrrGMGAADiTq8GoldffVUDBgzoza8EAADocz26ZTZ37tyQz8YY1dbWav/+/brnnnt6pWEAAAD9pUeByOv1hnx2u9267LLLdN999yk/P79XGgYAANBfehSIHnvssd5uBwAAQNT0KBCdU1VVpaNHj8rlcmns2LG66qqreqtdAAAA/aZHgai+vl633nqrXnrpJV188cUyxigQCOi6665TeXm5Lrnkkt5uJ7rJsiwZY7qsCQaD/dQaAABiW4+eMissLFRjY6OOHDmiv//972poaNDhw4fV2NioZcuW9XYb0U2WZWlE1mglJyd3uXk8nrMndJ2bAABIeD0aIdqxY4d27dqlyy+/3N43duxYPfjgg0yqjgHGGNWdPK5bHqyUy9155rXOtOnJwmnkIQCA4/UoEFmWpZSUlHb7U1JSZFnWBTcKvcPldsvtTur0uHFzywwAAKmHt8ymTZumu+66SydPnrT3nThxQt/5znc0ffr0XmscAABAf+hRICorK1NTU5PGjBmjj33sY/r4xz+u7OxsNTU1qbS0tLfbCAAA0Kd6dMssKytLBw4cUEVFhf74xz/KGKOxY8dqxowZvd0+AACAPtetEaIXXnhBY8eOVWNjoyRp5syZKiws1LJly3T11Vfriiuu0CuvvNInDQUAAOgr3QpEGzZs0O23367Bgwe3O+b1erV48WKtW7eu1xoHAADQH7oViH7/+9/r+uuv7/R4fn6+qqqqLrhRAAAA/albgei9997r8HH7c5KTk/X+++9fcKMAAAD6U7cC0YgRI3To0KFOj7/55psaPnz4BTcKwFnBYDDsxru/AODCdSsQff7zn9cPf/hDffTRR+2ONTc3695771VBQUGvNQ5wKmNZkjtJHo8n7BIsI7JGE4oA4AJ167H7H/zgB3rqqaf0iU98QkuXLtVll10ml8ulo0eP6sEHH1QwGNTq1av7qq2AgxjJCurmskq5kzr/d4uxLD2xZErYhXwBAF3rViDKzMzUnj179O1vf1urVq2yfwi7XC7NmjVLDz30kDIzM/ukoYAThVt+hXEhAOgd3X4x4+jRo/Xss8+qoaFB77zzjowxysnJ0ZAhQ/qifQAAAH2uR2+qlqQhQ4bo6quv7s22AAAAREWP1jIDAABIJDEfiE6cOKGvfOUrysjI0EUXXaRPfepTIS9/NMaouLhYfr9fAwcO1NSpU3XkyJGQ72hpaVFhYaGGDh2qQYMGac6cOTp+/Hh/dwUAAMSomA5EDQ0N+sxnPqOUlBT97ne/0x/+8Ac98MADuvjii+2atWvXat26dSorK9O+ffvk8/k0c+ZMNTU12TVFRUXavn27ysvLtXv3bp06dUoFBQUKBoNR6BUAAIg1PZ5D1B/uv/9+ZWVl6bHHHrP3jRkzxv6zMUYbNmzQ6tWrNXfuXEnSli1blJmZqW3btmnx4sUKBALatGmTHn/8cc2YMUOStHXrVmVlZWnXrl2aNWtWh393S0uLWlpa7M/nFrQFAACJJ6ZHiJ555hlNmDBBX/ziFzVs2DBdddVVevTRR+3j1dXVqqurU35+vr3P4/FoypQp2rNnjySpqqpKbW1tITV+v1+5ubl2TUdKSkrk9XrtLSsrqw96CAAAYkFMB6I///nPevjhh5WTk6PnnntOd9xxh5YtW6Zf/vKXkqS6ujpJavfuo8zMTPtYXV2dUlNT270W4J9rOrJq1SoFAgF7q6mp6c2uAQCAGBLTt8wsy9KECRO0Zs0aSdJVV12lI0eO6OGHH9ZXv/pVu87lcoWcZ4xpt+984Wo8Ho88Hs8FtB4AAMSLmB4hGj58uMaOHRuy7/LLL9e7774rSfL5fJLUbqSnvr7eHjXy+XxqbW1VQ0NDpzUAAMDZYjoQfeYzn9Fbb70Vsu9Pf/qTRo8eLUnKzs6Wz+dTRUWFfby1tVWVlZWaPHmyJCkvL08pKSkhNbW1tTp8+LBdA8S7YDAYdmMBWADoXEzfMvvOd76jyZMna82aNZo3b55ef/11bdy4URs3bpR09lZZUVGR1qxZo5ycHOXk5GjNmjW66KKLNH/+fEmS1+vVokWLtGLFCmVkZCg9PV0rV67UuHHj7KfOgHhlLEtyJ0V0e9fnH6kTNcfkdsf0v4MAICpiOhBdffXV2r59u1atWqX77rtP2dnZ2rBhg2677Ta75u6771Zzc7PuvPNONTQ0aOLEidq5c6fS0tLsmvXr1ys5OVnz5s1Tc3Ozpk+frs2bNyspqfNFM4H4YCQrqJvLKuVO6jzoGMvSE0um2AsyAwBCxXQgkqSCggIVFBR0etzlcqm4uFjFxcWd1gwYMEClpaUqLS3tgxYC0edyu+V2dx7wuVkGAF1j7BwAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADgegQgAADhezK9lBqD3BIPBsDUul0tuN/9WAuAsBCLAAYxlSe4keTyesLU+/0idqDlGKALgKAQiwBGMZAV1c1ml3EmdBx1jWXpiyRQZY/qxbQAQfQQiwEFcbrfc7qROj1v92BYAiCWMiQMAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMcjEAEAAMfjTdUA2olkEViJhWABJA4CEQBbdxaBlVgIFkDiIBAB+CeRLQIrsRAsgMRCIALQTrhFYCUWggWQWBjnBgAAjkcgAgAAjkcgAgAAjkcgAgAAjkcgAgAAjkcgAgAAjkcgAgAAjkcgAgAAjseLGQH0OcuyInqjNWujAYgWfvIA6FOWZWlE1mglJyeH3UZkjZZl8Q5sAP2PESIAFyQYDIY9XnfyuG55sFKuLkZ/WBsNQDQRiAD0iLEsyZ0kj8cTUb3L1fX6aIwLAYgmAhGAHjKSFdTNZZVyJ3U+8mOdadOThdPEuA+AWBZXc4hKSkrkcrlUVFRk7zPGqLi4WH6/XwMHDtTUqVN15MiRkPNaWlpUWFiooUOHatCgQZozZ46OHz/ez60HEpPLfXbkp7Otq9tkABAr4uYn1b59+7Rx40ZdeeWVIfvXrl2rdevWqaysTPv27ZPP59PMmTPV1NRk1xQVFWn79u0qLy/X7t27derUKRUUFISd+wAAAJwhLgLRqVOndNttt+nRRx/VkCFD7P3GGG3YsEGrV6/W3LlzlZubqy1btujDDz/Utm3bJEmBQECbNm3SAw88oBkzZuiqq67S1q1bdejQIe3atStaXQIAADEkLgLRkiVLdMMNN2jGjBkh+6urq1VXV6f8/Hx7n8fj0ZQpU7Rnzx5JUlVVldra2kJq/H6/cnNz7ZqOtLS0qLGxMWQDAACJKeYnVZeXl+vAgQPat29fu2N1dXWSpMzMzJD9mZmZOnbsmF2TmpoaMrJ0rubc+R0pKSnRj370owttPgAAiAMxPUJUU1Oju+66S1u3btWAAQM6rXO5XCGfjTHt9p0vXM2qVasUCATsraampnuN7yOWZSkYDIbdAABA5GI6EFVVVam+vl55eXn2m2wrKyv185//XMnJyfbI0PkjPfX19fYxn8+n1tZWNTQ0dFrTEY/Ho8GDB4ds0RbpG3/t98LwnDMAABGJ6Vtm06dP16FDh0L2ff3rX9cnP/lJfe9739Oll14qn8+niooKXXXVVZKk1tZWVVZW6v7775ck5eXlKSUlRRUVFZo3b54kqba2VocPH9batWv7t0MXyBgT0Rt/ee8LAADdE9OBKC0tTbm5uSH7Bg0apIyMDHt/UVGR1qxZo5ycHOXk5GjNmjW66KKLNH/+fEmS1+vVokWLtGLFCmVkZCg9PV0rV67UuHHj2k3Sjhfn3vvSGePmlhkAAN0R04EoEnfffbeam5t15513qqGhQRMnTtTOnTuVlpZm16xfv17JycmaN2+empubNX36dG3evFlJSZ2HCgAA4BxxF4heeumlkM8ul0vFxcUqLi7u9JwBAwaotLRUpaWlfds4AAAQl2J6UjUAAEB/IBABAADHIxABAADHIxABAADHIxABAADHIxABAADHi7vH7gHAsiwZE9m72F0ul9xdvNkdACRGiADEmUjX9Du3jcgaLcuyot1sADGOESIAcSXSNf0kyViWnlgyJeLRJADORSACEFOCwa7X4jt3PNyafpLEuBCASBGIAMQEY1mSO0kejyfCE/q2PQCchUAEIEYYyQrq5rJKuZM6vxVmnWnTk4XTyEMAehWBCEBMCXcrzLi7vqUGAD3BU2YAAMDxCEQAAMDxCEQAAMDxCEQAAMDxCEQAAMDxCEQAAMDxCEQAAMDxeA8RAOjsorGRrHnmcrnkDrOGGoD4QyACkPDCrY9mWZZGjclW3ckTYb/L5x+pEzXHCEVAgiEQAUhY3V0f7ZaySrm6WDbEWJaeWDIlopEkAPGFQAQggXVvfTSFWTbE6osmAogJBCIACY/10QCEw01wAADgeAQiAADgeAQiAADgeAQiAADgeAQiAADgeAQiAADgeAQiAADgeLyHCAC6KdxSIBJrngHxhkAEABHqzlIgrHkGxBcCEQBELLKlQFjzDIg/BCIA6KZwS4Gw5hkQfxjLBQAAjscIEQD0kUgmX0tMwAZiAYEIAHpZdyZfS0zABmIBgQgAel1kk68lJmADsYJABAB9JNzka4kJ2ECsiOnx2ZKSEl199dVKS0vTsGHDdNNNN+mtt94KqTHGqLi4WH6/XwMHDtTUqVN15MiRkJqWlhYVFhZq6NChGjRokObMmaPjx4/3Z1cAAEAMi+lAVFlZqSVLlmjv3r2qqKjQmTNnlJ+fr9OnT9s1a9eu1bp161RWVqZ9+/bJ5/Np5syZampqsmuKioq0fft2lZeXa/fu3Tp16pQKCgoinvAIAAASW0zfMtuxY0fI58cee0zDhg1TVVWVrr32WhljtGHDBq1evVpz586VJG3ZskWZmZnatm2bFi9erEAgoE2bNunxxx/XjBkzJElbt25VVlaWdu3apVmzZvV7vwAAQGyJ6RGi8wUCAUlSenq6JKm6ulp1dXXKz8+3azwej6ZMmaI9e/ZIkqqqqtTW1hZS4/f7lZuba9d0pKWlRY2NjSEbAABITHETiIwxWr58uT772c8qNzdXklRXVydJyszMDKnNzMy0j9XV1Sk1NVVDhgzptKYjJSUl8nq99paVldWb3QEAADEkbgLR0qVL9eabb+rXv/51u2MulyvkszGm3b7zhatZtWqVAoGAvdXU1PSs4QAAIObFRSAqLCzUM888oxdffFEjR4609/t8PklqN9JTX19vjxr5fD61traqoaGh05qOeDweDR48OGQDAACJKaYDkTFGS5cu1VNPPaUXXnhB2dnZIcezs7Pl8/lUUVFh72ttbVVlZaUmT54sScrLy1NKSkpITW1trQ4fPmzXAEC0BYPBsJtl8dYioK/E9FNmS5Ys0bZt2/Q///M/SktLs0eCvF6vBg4cKJfLpaKiIq1Zs0Y5OTnKycnRmjVrdNFFF2n+/Pl27aJFi7RixQplZGQoPT1dK1eu1Lhx4+ynzgAgWrqzzAdLfAB9J6YD0cMPPyxJmjp1asj+xx57TF/72tckSXfffbeam5t15513qqGhQRMnTtTOnTuVlpZm169fv17JycmaN2+empubNX36dG3evFlJSV2/QRYA+l5ky3ywxAfQt2I6EEXyH77L5VJxcbGKi4s7rRkwYIBKS0tVWlrai60DgN4TbpmP7twssywr4p+fjDYBZ8V0IAIAhAr3hn3LsjRqTLbqTp4I+13cggP+gUAEAHGgO3ONJOmWskq5IrgFd+bMmYimDzCahERHIAKAuBDZXCPrTJueLJwmhbkFF+xmwGI0CYmOQAQAcSTcXCPjjnTR6sgClsSEbjgDgQgAHCxcwJK6N6EbiFeMfQIAAMcjEAEAAMcjEAEAAMdjDhEAoNfwUkjEK/7fCADoFZZlaUTWaCUnJ4fdRmSNZrFaxBRGiAAAEQn3luxgMKi6k8d1y4OVcnUx+sNj/IhFBCIAQJe6+5Zsl6v31mUD+guBCAAQRvfeks24D+IRgSgGRDoJMdxwNQD0pd57S/ZZkfxMY/I1+guBKMrOTUKsO3k88pP45xeAONadW3CsoYb+QiCKMmNMRJMQJYajASSKyG7BdXfyNY/840IQiGJEJOsJdXc4GgBiWbife+cmX0dya82yLI0ak626kyfC1jLqhI4QiAAAMam7T7dJ0i1llXL14qgTnINABACIUZHdWpP+MaVAEY46AecjEAEAYhpTCtAfCEQAAMfhkX+cj0AEAHAMHvlHZwhEAAAH6ZtH/hH/CEQAAMfpzUf+I721Ful7krrzneg9BCIAAM7TnVtrmcNHqOZYdZcBpjvvSZIiv13Hyyh7D4EIAIB2Il3Q9oyevGuGUlNTI/rWcO9JkiK/XdedpZ+YDxUegQgAgE5EtKBtRMEpsvckSZG/KynSpZ+YDxUZAhEAABcoouAUpb+bl1FGhkAEAEAMCjehO5IJ34gcgQgAgBjS7TXcIrwTxssou0YgAgAgpkQ6ofvsvKRweYiXUUaGQAQAQAzqvXlJffMyykge+Y+n23oEIgAAHKA3J19355F/SRHf1osmAhEAALBFMqoTDAYjeuQ/0tt6sYBABAAAuj+ZW5LLFb3XDfQ2AhEAAFCkc42k+Br5iRSBCAAA2MLNNZLia+QnUs57rg4AAOA8BCIAAOB4BCIAAOB4BCIAAOB4BCIAAOB4jgpEDz30kLKzszVgwADl5eXplVdeiXaTAABADHBMIPrNb36joqIirV69Wm+88YY+97nPafbs2Xr33Xej3TQAABBljnkP0bp167Ro0SJ985vflCRt2LBBzz33nB5++GGVlJS0q29paVFLS4v9ORAISJIaGxt7tV3nXpHeerqpy9efS2dfhHWuNtwKyNGoo420MZbqaCNtjMc6p7bRWGdXUmtsbFRSUtfvQOquc7+3wy5aaxygpaXFJCUlmaeeeipk/7Jly8y1117b4Tn33nuv0dnl6NjY2NjY2NjifKupqekyKzhihOivf/2rgsGgMjMzQ/ZnZmaqrq6uw3NWrVql5cuX258ty9Lf//53ZWRkyOVyXXCbGhsblZWVpZqaGg0ePPiCvy8WJXofE71/En1MBIneP4k+JoK+7J8xRk1NTfL7/V3WOSIQnXN+kDHGdBpuPB5PuwXuLr744l5v0+DBgxPy/9z/LNH7mOj9k+hjIkj0/kn0MRH0Vf+8Xm/YGkdMqh46dKiSkpLajQbV19e3GzUCAADO44hAlJqaqry8PFVUVITsr6io0OTJk6PUKgAAECscc8ts+fLlWrBggSZMmKBJkyZp48aNevfdd3XHHXdEpT0ej0f33ntvu9tyiSTR+5jo/ZPoYyJI9P5J9DERxEL/XMaEew4tcTz00ENau3atamtrlZubq/Xr1+vaa6+NdrMAAECUOSoQAQAAdMQRc4gAAAC6QiACAACORyACAACORyACAACORyCKgoceekjZ2dkaMGCA8vLy9Morr0S7ST1WXFwsl8sVsvl8Pvu4MUbFxcXy+/0aOHCgpk6dqiNHjkSxxeG9/PLLuvHGG+X3++VyufT000+HHI+kTy0tLSosLNTQoUM1aNAgzZkzR8ePH+/HXnQuXP++9rWvtbum11xzTUhNLPevpKREV199tdLS0jRs2DDddNNNeuutt0Jq4v0aRtLHeL+ODz/8sK688kr7zcWTJk3S7373O/t4vF/DcP2L9+vXkZKSErlcLhUVFdn7Yuk6Eoj62W9+8xsVFRVp9erVeuONN/S5z31Os2fP1rvvvhvtpvXYFVdcodraWns7dOiQfWzt2rVat26dysrKtG/fPvl8Ps2cOVNNTU1RbHHXTp8+rfHjx6usrKzD45H0qaioSNu3b1d5ebl2796tU6dOqaCgQMFgsL+60alw/ZOk66+/PuSaPvvssyHHY7l/lZWVWrJkifbu3auKigqdOXNG+fn5On36tF0T79cwkj5K8X0dR44cqZ/85Cfav3+/9u/fr2nTpukLX/iC/csy3q9huP5J8X39zrdv3z5t3LhRV155Zcj+mLqOF7aOPLrr05/+tLnjjjtC9n3yk5803//+96PUogtz7733mvHjx3d4zLIs4/P5zE9+8hN730cffWS8Xq955JFH+qmFF0aS2b59u/05kj598MEHJiUlxZSXl9s1J06cMG632+zYsaPf2h6J8/tnjDELFy40X/jCFzo9J576Z4wx9fX1RpKprKw0xiTeNTSmfR+NSbzraIwxQ4YMMf/5n/+ZkNfQmH/0z5jEun5NTU0mJyfHVFRUmClTppi77rrLGBN7/y0yQtSPWltbVVVVpfz8/JD9+fn52rNnT5RadeHefvtt+f1+ZWdn69Zbb9Wf//xnSVJ1dbXq6upC+uvxeDRlypS47W8kfaqqqlJbW1tIjd/vV25ubtz0+6WXXtKwYcP0iU98Qrfffrvq6+vtY/HWv0AgIElKT0+XlJjX8Pw+npMo1zEYDKq8vFynT5/WpEmTEu4ant+/cxLl+i1ZskQ33HCDZsyYEbI/1q6jY5buiAV//etfFQwG2y0om5mZ2W7h2XgxceJE/fKXv9QnPvEJvffee/r3f/93TZ48WUeOHLH71FF/jx07Fo3mXrBI+lRXV6fU1FQNGTKkXU08XOfZs2fri1/8okaPHq3q6mrdc889mjZtmqqqquTxeOKqf8YYLV++XJ/97GeVm5srKfGuYUd9lBLjOh46dEiTJk3SRx99pH/5l3/R9u3bNXbsWPsXYbxfw876JyXG9ZOk8vJyHThwQPv27Wt3LNb+WyQQRYHL5Qr5bIxpty9ezJ492/7zuHHjNGnSJH3sYx/Tli1b7AmAidTfc3rSp3jp95e+9CX7z7m5uZowYYJGjx6t//u//9PcuXM7PS8W+7d06VK9+eab2r17d7tjiXINO+tjIlzHyy67TAcPHtQHH3ygJ598UgsXLlRlZaV9PN6vYWf9Gzt2bEJcv5qaGt11113auXOnBgwY0GldrFxHbpn1o6FDhyopKaldqq2vr2+XkOPVoEGDNG7cOL399tv202aJ1N9I+uTz+dTa2qqGhoZOa+LJ8OHDNXr0aL399tuS4qd/hYWFeuaZZ/Tiiy9q5MiR9v5Euoad9bEj8XgdU1NT9fGPf1wTJkxQSUmJxo8fr5/97GcJcw07619H4vH6VVVVqb6+Xnl5eUpOTlZycrIqKyv185//XMnJyXY7Y+U6Eoj6UWpqqvLy8lRRURGyv6KiQpMnT45Sq3pXS0uLjh49quHDhys7O1s+ny+kv62traqsrIzb/kbSp7y8PKWkpITU1NbW6vDhw3HZ77/97W+qqanR8OHDJcV+/4wxWrp0qZ566im98MILys7ODjmeCNcwXB87Em/XsSPGGLW0tCTENezIuf51JB6v3/Tp03Xo0CEdPHjQ3iZMmKDbbrtNBw8e1KWXXhpb17FXp2gjrPLycpOSkmI2bdpk/vCHP5iioiIzaNAg85e//CXaTeuRFStWmJdeesn8+c9/Nnv37jUFBQUmLS3N7s9PfvIT4/V6zVNPPWUOHTpkvvzlL5vhw4ebxsbGKLe8c01NTeaNN94wb7zxhpFk1q1bZ9544w1z7NgxY0xkfbrjjjvMyJEjza5du8yBAwfMtGnTzPjx482ZM2ei1S1bV/1ramoyK1asMHv27DHV1dXmxRdfNJMmTTIjRoyIm/59+9vfNl6v17z00kumtrbW3j788EO7Jt6vYbg+JsJ1XLVqlXn55ZdNdXW1efPNN82//du/GbfbbXbu3GmMif9r2FX/EuH6deafnzIzJrauI4EoCh588EEzevRok5qaav71X/815FHZePOlL33JDB8+3KSkpBi/32/mzp1rjhw5Yh+3LMvce++9xufzGY/HY6699lpz6NChKLY4vBdffNFIarctXLjQGBNZn5qbm83SpUtNenq6GThwoCkoKDDvvvtuFHrTXlf9+/DDD01+fr655JJLTEpKihk1apRZuHBhu7bHcv866psk89hjj9k18X4Nw/UxEa7jN77xDfvn5CWXXGKmT59uhyFj4v8adtW/RLh+nTk/EMXSdXQZY0zvjjkBAADEF+YQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAxyMQAQAAx/t/4wQNfiixBGoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18756, 18756)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hpwad WAD COM hpwbe007 wad Need help writing MS EXCEL macrofor student begin for assignments begin score score end score foundend could not you simply use minas you use sumand than subtract from sumgermany fromut fritze waldbronn analytic division waldbronn germany ',\n",
       " 'ecst csuchico edu netcom netcom vega iii enrico tmc edu netcom netcom ecst csuchico edu To be or Not to be a Disaster article article article not all you are apparently just another member the leftnot all not member the rightor even centerin fact not consider myself very religious all this will probably result flames now fact phil you should leave religion out just clouds the issue the religous left worships trees rivers the planet and hates people and the religious right worships engines smokestacks landfills and hates people what does this name calling have with anything you are claiming about the truth environmental disaster nothing that have read this thread nor heard from anyone have talked would suggest that people fit the definition you give the religious left comeoff prime motivation for protecting our environment that people can continue live healthily just disagree what necessary maintaining healthy environment for peopleshow all these environmental disasters most them are not and the natural disasters have had individually far outweigh the man made ones russis response deleted save space guess you missed the newspaper articles this week about presenting evidnce the issue the valdezincident seems that mostly recovered despite the leftis cries hundreds years what they have already repaired that old hulk wow suppose you mean the alaskanshores that were devastated the valdezaccident have not seen the articles what they say exactly has mostly all the ocean and shore life returned the sands are mostly clean they were before the microbial samples are mostly back normal balance the fish and fowl populations have mostly returned what then again the leftclaimed would take yearsb put out the kuwaitoil fires you should face the facts was not and not environmental disaster nor even problem nor and tmiand acid rain killing trees and not problem would you move would imagine there some cheap property available the naturally occurring catastrophic events disasters that destroy property not usually leave toxic wastes that prevent people from building their lives there the man made disasters cause death and make area unliveable far beyond the initial event there are actually people that still believe was some kind environmental disaster these opinions are mineand you can not have not know why but seem expect serious discussion the net ',\n",
       " 'csugrad edu ofa123 fidonet org csugrad edu csugrad edu csugrad edu Church o Satan was islamic authority sic over womenyeah hilarious satanists believe satanis god but not the only god satanis part therefore one can not reasonably worship satan without acknowledging the existence christiangod satanists see satanas their master and they see and satanas kmadversaries similar power satanists believe the kmeventual overthrow god and transfer all power their master great many satanists notbelieve satansome some not would far assert that most orthodox satanists not worship satan but rather worship self hear laveysay only idiots and fools believe satanand allah knew that suckers are born every minute maximus interpretation satanismhas always puzzled read his satanic bible few years ago for social studies project well book called the cult americathe latter included very interesting interview with the popein which did indeed say that was merely instrument for one realize the self when refer satanism referring the mishmash rural satanic ritualism and witchcraft which existed before the church satani not consider laveyis church all orthodox nor consider its followers satanists laveycombined the philosophies and slapped some religious doctrine added little touch and christened his creation the churchof satanno doubt the title was calculated attempt attract attention suppose could have just easily called the churchof any rate worked its heyday the had huge following including such hollywood celebrities and find the idea satanist not believing satanabout credible christiannot believing christbut you include the churchof satanthen suppose need alter definition dictionary and the will have the same computer science department virginia tech blacksburg ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,idx in a.items():\n",
    "    if idx == 0:\n",
    "        print(idx,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = tf.keras.preprocessing.sequence.pad_sequences(data_X, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18756, 411)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1127,   416,     7,  5367,  1012,    23,   333,  1171,    28,\n",
       "         222, 37620,   768, 25072,   481,    22,  1226,   699,     2,\n",
       "          27,   731,  2943,  1401,   937,     6,    38,   333,  2464,\n",
       "          14,     8,  1337,   864,    43,  1428,     5,  5457,    10,\n",
       "        1191,    11,  8538,     1,  2066,    29,  2464,     1,    22,\n",
       "          29,    17,   678,     1,  1317,    17,     4,  1119,     1,\n",
       "        3536,     1,  1218,   559,    17,     1,  1218,  1119,     1,\n",
       "        3536,     1,  1317,     1,  6627,     1,  3562,   228,    17,\n",
       "       11726,   109,     2,     1, 22042,     1,  8539,    17,  3149,\n",
       "         177,   109,  8538,     4,  4754,  1617,    24,  1439,   307,\n",
       "        4489,    97,  4436,    71,  2170,   448,    87,   538,    10,\n",
       "         760, 37621,    28,   249,     6,    82,    29,   582,   371,\n",
       "           1,   752,    15,   820,    24,     4,  2379,   976,    11,\n",
       "        4980,  1002,   155,    41,    26,   485,   207,  1825,  3669,\n",
       "         747,     5,   190,   994,  4980,  1002,   377,  4980,  1002,\n",
       "          33, 11727,   126,  5203,    12,    10,   182,    13,    87,\n",
       "          86,    90,   332,   921,   421,  3020,   490, 37621,    27,\n",
       "       15451,    12,     4,    33,  2252,     1,  6076,     4,   455,\n",
       "           3,  4620,  4980,  1002,    33,    87,    86,    90,  1143,\n",
       "         834,    22,   312,  1861,  2380, 37622,  5751,     3,  2321,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "oneHot = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_np = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comp.os.ms-windows.misc', 'comp.sys.mac.hardware',\n",
       "       'comp.sys.mac.hardware', 'rec.motorcycles', 'rec.autos',\n",
       "       'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt',\n",
       "       'comp.sys.mac.hardware', 'talk.politics.mideast'], dtype='<U24')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = oneHot.fit_transform(y_np.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
       "        'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "        'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
       "        'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
       "        'sci.electronics', 'sci.med', 'sci.space',\n",
       "        'soc.religion.christian', 'talk.politics.guns',\n",
       "        'talk.politics.mideast', 'talk.politics.misc',\n",
       "        'talk.religion.misc'], dtype='<U24')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHot.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18756, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = y_onehot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(padded_data,y_final,stratify=y_final,test_size=0.2)\n",
    "X_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,stratify=y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13503, 411), (3752, 411), (13503, 20), (3752, 20))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:12, 32551.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "glove_emb = loadGloveModel('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = glove_emb['the'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(a)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115655"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding = np.zeros((vocab_size,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "not_found = [] \n",
    "for word,idx in a.items():\n",
    "    if glove_emb.get(word) is not None:\n",
    "        count+=1\n",
    "        embeding[idx] = glove_emb[word]\n",
    "    else:\n",
    "        not_found.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45042"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['universityof',\n",
       " 'utexas',\n",
       " 'optilink',\n",
       " 'iastate',\n",
       " 'udel',\n",
       " 'uchicago',\n",
       " 'uxa',\n",
       " 'umich',\n",
       " 'colostate',\n",
       " 'psuvm',\n",
       " 'fnal',\n",
       " 'xterm',\n",
       " 'dseg',\n",
       " 'uoknor',\n",
       " 'utoronto',\n",
       " 'ingr',\n",
       " 'solntze',\n",
       " 'uwaterloo',\n",
       " 'vnet',\n",
       " 'acns',\n",
       " 'uicvm',\n",
       " 'bony1',\n",
       " 'bhj',\n",
       " 'rtsg',\n",
       " 'sdpa',\n",
       " 'wustl',\n",
       " 'clh',\n",
       " 'calpoly',\n",
       " 'rwing',\n",
       " 'cunixb',\n",
       " 'ulowell',\n",
       " 'utkvm1',\n",
       " 'thanx',\n",
       " 'dividian',\n",
       " 'enet',\n",
       " 'dscomsa',\n",
       " 'mcrcim',\n",
       " 'okstate',\n",
       " 'cbnewsj',\n",
       " 'ccwf',\n",
       " 'uky',\n",
       " 'osrhe',\n",
       " 'd012s658',\n",
       " 'ultb',\n",
       " 'ucsu',\n",
       " 'forsale',\n",
       " 'turksand',\n",
       " 'colormap',\n",
       " 'msdos',\n",
       " 'umanitoba',\n",
       " 'spdcc',\n",
       " 'cnsvax',\n",
       " 'uwec',\n",
       " 'departmentof',\n",
       " 'atlantaga',\n",
       " 'thanksin',\n",
       " 'royalroads',\n",
       " 'cunyvm',\n",
       " 'ctron',\n",
       " 'ucalgary',\n",
       " 'bdis',\n",
       " 'xdm',\n",
       " 'uio',\n",
       " 'imho',\n",
       " 'latech',\n",
       " 'thanksfor',\n",
       " 'openwindows',\n",
       " 'mchp',\n",
       " 'dbstu1',\n",
       " 'mksol',\n",
       " 'isarticle',\n",
       " 'uxh',\n",
       " 'msus',\n",
       " 'ux4',\n",
       " 'msstate',\n",
       " 'washingtondc',\n",
       " 'yfn',\n",
       " 'utzoo',\n",
       " 'buphy',\n",
       " 'ucdavis',\n",
       " 'chinet',\n",
       " 'godis',\n",
       " 'survivorsarticle',\n",
       " 'isabus',\n",
       " 'okcforum',\n",
       " 'ux1',\n",
       " 'acpub',\n",
       " 'newtestament',\n",
       " 'centerfor',\n",
       " 'morearticle',\n",
       " 'aisun3',\n",
       " 'lerc',\n",
       " 'umcc',\n",
       " 'lciii',\n",
       " 'idbsu',\n",
       " 'jxp',\n",
       " 'pixmap',\n",
       " 'percentagearticle',\n",
       " 'ocom',\n",
       " 'cmuvm',\n",
       " 'turkishsoul',\n",
       " 'xview',\n",
       " 'uokmax',\n",
       " 'kimbark',\n",
       " 'clarku',\n",
       " 'ualberta',\n",
       " 'doesanyone',\n",
       " 'questionarticle',\n",
       " 'jhunix',\n",
       " 'salehave',\n",
       " 'bibleis',\n",
       " 'batffbi',\n",
       " 'gtoal',\n",
       " 'linknet',\n",
       " 'harddisk',\n",
       " 'ubvmsb',\n",
       " 'uug',\n",
       " 'ncratl',\n",
       " 'ibmpc',\n",
       " 'cbnews',\n",
       " 'sunysb',\n",
       " 'cananyone',\n",
       " 'bcstec',\n",
       " 'dsinc',\n",
       " 'inqmind',\n",
       " 'libxmu',\n",
       " 'gothamcity',\n",
       " 'imag',\n",
       " 'atheistsarticle',\n",
       " 'israeland',\n",
       " 'psilink',\n",
       " 'netnews',\n",
       " 'nysernet',\n",
       " 'exnet',\n",
       " 'lmsc',\n",
       " 'apana',\n",
       " 'jyusenkyou',\n",
       " 'acad3',\n",
       " 'yorku',\n",
       " 'nodak',\n",
       " 'cec1',\n",
       " 'krillean',\n",
       " 'warningplease',\n",
       " 'mizzou1',\n",
       " 'uiowa',\n",
       " 'earthis',\n",
       " 'nodomain',\n",
       " 'cview',\n",
       " 'bmug',\n",
       " 'utkvx',\n",
       " 'fbihh',\n",
       " 'christis',\n",
       " 'nosubdomain',\n",
       " 'vax1',\n",
       " 'contrib',\n",
       " 'ocis',\n",
       " 'ulkyvx',\n",
       " 'unx',\n",
       " 'stpl',\n",
       " 'fbiand',\n",
       " 'catbyte',\n",
       " 'ethz',\n",
       " 'megatek',\n",
       " 'unisql',\n",
       " 'unomaha',\n",
       " 'yorkhave',\n",
       " 'ozonehole',\n",
       " 'americais',\n",
       " 'cardarticle',\n",
       " 'nextwork',\n",
       " 'muslimpeople',\n",
       " 'scsidrive',\n",
       " 'usgovernment',\n",
       " 'dosand',\n",
       " 'bmwr',\n",
       " 'sfasu',\n",
       " 'ruu',\n",
       " 'updatearticle',\n",
       " 'csd4',\n",
       " 'queensu',\n",
       " 'ukans',\n",
       " 'mcimail',\n",
       " 'uoregon',\n",
       " 'cwis',\n",
       " 'ccsvax',\n",
       " 'batfis',\n",
       " 'turkishand',\n",
       " 'trincoll',\n",
       " 'kfu',\n",
       " 'nswc',\n",
       " 'gozer',\n",
       " 'monu6',\n",
       " 'ksand',\n",
       " 'hpcc01',\n",
       " 'cmhnet',\n",
       " 'bosniansarticle',\n",
       " 'instituteof',\n",
       " 'utxvms',\n",
       " 'nsais',\n",
       " 'rchland',\n",
       " 'organpipe',\n",
       " 'ritvax',\n",
       " 'calstate',\n",
       " 'endif',\n",
       " 'skndiv',\n",
       " 'utarlg',\n",
       " 'jarthur',\n",
       " 'campaign92',\n",
       " 'shaftdrives',\n",
       " 'scarolina',\n",
       " 'usaarticle',\n",
       " 'ubvms',\n",
       " 'cogsci',\n",
       " 'kaldis',\n",
       " 'auvm',\n",
       " 'houseoffice',\n",
       " 'clipperchip',\n",
       " 'superstitionarticle',\n",
       " 'compdyn',\n",
       " 'finabo',\n",
       " 'georgiaphone',\n",
       " 'israelto',\n",
       " 'cbnewsg',\n",
       " 'itarticle',\n",
       " 'b30',\n",
       " 'dgbt',\n",
       " 'espnis',\n",
       " 'israelhas',\n",
       " 'csugrad',\n",
       " 'hellarticle',\n",
       " 'fnalf',\n",
       " 'supportersarticle',\n",
       " 'cunixc',\n",
       " 'ccit',\n",
       " 'vax5',\n",
       " 'idecontroller',\n",
       " 'bibleand',\n",
       " 'readarticle',\n",
       " 'macalstr',\n",
       " 'usask',\n",
       " 'autoexec',\n",
       " 'imake',\n",
       " 'hpfcso',\n",
       " 'brocku',\n",
       " 'ufsa',\n",
       " 'idedrive',\n",
       " 'srl02',\n",
       " 'daysarticle',\n",
       " 'bookof',\n",
       " 'byuvm',\n",
       " 'csuchico',\n",
       " 'problemsarticle',\n",
       " 'asalasdpa',\n",
       " 'theporch',\n",
       " 'guvax',\n",
       " 'megatest',\n",
       " 'qdeck',\n",
       " 'mmalt',\n",
       " 'salefor',\n",
       " 'speedstar',\n",
       " 'playersarticle',\n",
       " 'aukuni',\n",
       " 'x11r5',\n",
       " 'ripem',\n",
       " 'dsg4',\n",
       " 'bchm',\n",
       " 'olwm',\n",
       " 'neoucom',\n",
       " 'ofcourse',\n",
       " 'skyarticle',\n",
       " 'fbiis',\n",
       " 'jewsarticle',\n",
       " 'uclink',\n",
       " 'cmptrc',\n",
       " 'medraut',\n",
       " 'cunixa',\n",
       " 'srl03',\n",
       " 'houseof',\n",
       " 'mprgate',\n",
       " 'lawarticle',\n",
       " 'rlk',\n",
       " 'mindlink',\n",
       " 'xwindows',\n",
       " 'zoologybetween',\n",
       " 'newshost',\n",
       " 'sol1',\n",
       " 'idacom',\n",
       " 'slmr',\n",
       " 'zoologykipling',\n",
       " 'newjersey',\n",
       " 'hezbollaharticle',\n",
       " 'lordis',\n",
       " 'keyseach',\n",
       " 'schoolof',\n",
       " 'utdallas',\n",
       " 'ultrapro',\n",
       " 'fbihad',\n",
       " 'cbnewsl',\n",
       " 'rtfm',\n",
       " 'kuhub',\n",
       " 'ftms',\n",
       " 'hallof',\n",
       " 'eeap',\n",
       " 'atlantageorgia',\n",
       " 'univof',\n",
       " 'decwrl',\n",
       " 'nasais',\n",
       " 'christianityarticle',\n",
       " 'batfand',\n",
       " 'cbnewsh',\n",
       " 'delcoelect',\n",
       " 'bosnianmuslims',\n",
       " 'cmkrnl',\n",
       " 'dns1',\n",
       " 'husc',\n",
       " 'wvnet',\n",
       " 'colormaps',\n",
       " 'americanpeople',\n",
       " 'vuse',\n",
       " 'pcis',\n",
       " 'timearticle',\n",
       " 'vak12ed',\n",
       " 'uswest',\n",
       " 'csuohio',\n",
       " 'ersys',\n",
       " 'releaseapril',\n",
       " 'tcpip',\n",
       " 'partof',\n",
       " 'keycode',\n",
       " 'sfsuvax1',\n",
       " '419article',\n",
       " 'fxwg',\n",
       " 'hplabs',\n",
       " 'ftpsite',\n",
       " 'oldtestament',\n",
       " 'photographyarticle',\n",
       " 'scsiis',\n",
       " 'vaxvms',\n",
       " 'wuarchive',\n",
       " 'ubvmsd',\n",
       " 'ilstu',\n",
       " 'bsuvc',\n",
       " 'twisto',\n",
       " 'ocunix',\n",
       " 'godarticle',\n",
       " 'mavenry',\n",
       " 'kentksand',\n",
       " 'bikenow',\n",
       " 'iiarticle',\n",
       " 'wkuvx1',\n",
       " 'neededarticle',\n",
       " 'kocrsv01',\n",
       " 'windowsarticle',\n",
       " 'atlastele',\n",
       " 'protectedarticle',\n",
       " 'nothe',\n",
       " 'qucdn',\n",
       " 'asuacad',\n",
       " 'mpce',\n",
       " 'tvtwm',\n",
       " 'gfxvpic',\n",
       " 'vxcrna',\n",
       " 'grebyn',\n",
       " 'cunixf',\n",
       " 'bigboote',\n",
       " 'altcit',\n",
       " 'xmu',\n",
       " 'vax2',\n",
       " 'salearticle',\n",
       " 'aludra',\n",
       " 'fs7',\n",
       " 'politicsand',\n",
       " 'jeesus',\n",
       " 'amesdryden',\n",
       " 'mbunix',\n",
       " 'techbook',\n",
       " 'rintintin',\n",
       " 'vmcms',\n",
       " 'faqfor',\n",
       " 'xwd',\n",
       " 'iscsvax',\n",
       " 'ke4zv',\n",
       " 'unixg',\n",
       " 'musicb',\n",
       " 'bigwpi',\n",
       " 'statesand',\n",
       " 'ifyou',\n",
       " 'macis',\n",
       " 'ftcollinsco',\n",
       " 'lafibm',\n",
       " 'qmw',\n",
       " 'dazixco',\n",
       " 'pinouts',\n",
       " 'neosoft',\n",
       " 'cbnewsk',\n",
       " 'queenscould',\n",
       " 'bronxaway',\n",
       " 'yorkny',\n",
       " 'columbiasc',\n",
       " 'nosc',\n",
       " 'compulink',\n",
       " 'infoserv',\n",
       " 'gizw',\n",
       " 'happenedarticle',\n",
       " 'hellis',\n",
       " 'christiansarticle',\n",
       " 'encryptionarticle',\n",
       " 'christianityis',\n",
       " 'fanatism',\n",
       " 'mfltd',\n",
       " 'batfagents',\n",
       " 'umontreal',\n",
       " 'countersteering_faq',\n",
       " 'jcpl',\n",
       " 'eecg',\n",
       " 'irzr17',\n",
       " 'vaxc',\n",
       " 'vesterman',\n",
       " 'watstar',\n",
       " 'scsicontroller',\n",
       " 'waii',\n",
       " 'wvnvms',\n",
       " 'nuy',\n",
       " 'philabs',\n",
       " 'islamiclaw',\n",
       " 'romdrive',\n",
       " 'wordfor',\n",
       " 'greeceand',\n",
       " 'traditionarticle',\n",
       " 'clesun',\n",
       " 'williampitt',\n",
       " 'gvg47',\n",
       " 'kuleuven',\n",
       " 'biblesays',\n",
       " 'witsend',\n",
       " 'balltown',\n",
       " 'macii',\n",
       " 'jhuapl',\n",
       " 'floptical',\n",
       " 'psygate',\n",
       " 'sys6626',\n",
       " 'dhhalden',\n",
       " 'utoledo',\n",
       " 'ghj',\n",
       " 'ofa123',\n",
       " 'brandnew',\n",
       " 'bibledoes',\n",
       " 'stortek',\n",
       " 'uhunix',\n",
       " 'snake2',\n",
       " 'ampr',\n",
       " 'atfis',\n",
       " 'libx',\n",
       " 'usand',\n",
       " 'advtech',\n",
       " 'catholicchurch',\n",
       " 'publickey',\n",
       " 'molestersarticle',\n",
       " 'sunis',\n",
       " 'thoughtsarticle',\n",
       " 'ivyleague',\n",
       " 'christianis',\n",
       " 'hirama',\n",
       " 'mswindows',\n",
       " 'turksin',\n",
       " 'ellisun',\n",
       " 'amibios',\n",
       " 'councilof',\n",
       " 'iranis',\n",
       " 'slee01',\n",
       " 'chpc',\n",
       " 'mtroyal',\n",
       " 'mbeckman',\n",
       " 'albnyvms',\n",
       " 'nuscc',\n",
       " 'macand',\n",
       " 'slacvm',\n",
       " 'mentorg',\n",
       " 'joesbar',\n",
       " 'cbnewsc',\n",
       " 'fwi',\n",
       " 'wheeliesarticle',\n",
       " 'pixmaps',\n",
       " 'imat',\n",
       " 'faqis',\n",
       " 'thisarticle',\n",
       " 'uncecs',\n",
       " 'desqviewx',\n",
       " 'vpnet',\n",
       " 'xterminal',\n",
       " 'testarticle',\n",
       " 'etcarticle',\n",
       " 'tnet',\n",
       " 'uceng',\n",
       " 'sccsi',\n",
       " 'srgenprp',\n",
       " 'chiparticle',\n",
       " 'candidayeast',\n",
       " 'oortcloud',\n",
       " 'russiaand']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = padded_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(in_len,),dtype= tf.int32)\n",
    "\n",
    "embbeding_layer = tf.keras.layers.Embedding(input_dim= vocab_size,output_dim = d,weights=[embeding],input_length=in_len, trainable=False)(input_layer)\n",
    "\n",
    "\n",
    "layer_1_conv_4 = tf.keras.layers.Conv1D(filters = 5, kernel_size = 3, strides=1, activation='relu')(embbeding_layer)\n",
    "layer_1_conv_2 = tf.keras.layers.Conv1D(filters = 5, kernel_size = 2, strides=1, activation='relu')(embbeding_layer)\n",
    "layer_1_conv_1 = tf.keras.layers.Conv1D(filters = 5, kernel_size = 1, strides=1, activation='relu')(embbeding_layer)\n",
    "\n",
    "layer_2_concat = tf.keras.layers.concatenate([layer_1_conv_4, layer_1_conv_2, layer_1_conv_1],axis=1)\n",
    "max_pool_layer_1 = tf.keras.layers.MaxPool1D( pool_size=7,strides=1)(layer_2_concat)\n",
    "\n",
    "layer_2_conv_4 = tf.keras.layers.Conv1D(filters = 10, kernel_size = 4, strides=1, activation='relu')(max_pool_layer_1)\n",
    "layer_2_conv_2 = tf.keras.layers.Conv1D(filters = 10, kernel_size = 3, strides=1, activation='relu')(max_pool_layer_1)\n",
    "layer_2_conv_1 = tf.keras.layers.Conv1D(filters = 10, kernel_size = 2, strides=1, activation='relu')(max_pool_layer_1)\n",
    "\n",
    "layer_2_concat = tf.keras.layers.concatenate([layer_2_conv_4, layer_2_conv_2, layer_2_conv_1],axis=1)\n",
    "max_pool_layer_2 = tf.keras.layers.MaxPool1D( pool_size = 5,strides=1)(layer_2_concat)\n",
    "\n",
    "layer_2_conv_f = tf.keras.layers.Conv1D(filters = 1, kernel_size = 10, strides=1, activation='relu')(max_pool_layer_2)\n",
    "flat_layer = tf.keras.layers.Flatten()(layer_2_conv_f)\n",
    "\n",
    "dense_layer_1 = tf.keras.layers.Dense(32,activation='relu')(flat_layer)\n",
    "dropout_layer = tf.keras.layers.Dropout(.5)(dense_layer_1)\n",
    "output_layer = tf.keras.layers.Dense(20,activation='softmax')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(input_layer,output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,momentum = 0.7)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test = tf.math.add(input_layer,0)\n",
    "test_model = tf.keras.Model(input_layer,test)\n",
    "test_model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_vec = np.zeros(shape=(411,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_vec[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = tf.keras.layers.Embedding(input_dim= vocab_size,output_dim = d,weights=[embeding],input_length=in_len, trainable=False)(test_model.predict(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one = output[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one_emb = embeding[1,:].astype('float32')\n",
    "one_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a,b in zip (one,one_emb):\n",
    "    if a != b:\n",
    "        print(\"Error : \",a ,b)\n",
    "        break\n",
    "print(\"Sucess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array_equal(embeding[1,:] , one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 411)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 411, 100)     11565500    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 409, 5)       1505        ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 410, 5)       1005        ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 411, 5)       505         ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1230, 5)      0           ['conv1d_7[0][0]',               \n",
      "                                                                  'conv1d_8[0][0]',               \n",
      "                                                                  'conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 1224, 5)     0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 1221, 10)     210         ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 1222, 10)     160         ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 1223, 10)     110         ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 3666, 10)     0           ['conv1d_10[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]',              \n",
      "                                                                  'conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 3662, 10)    0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 3653, 1)      101         ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3653)         0           ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           116928      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 20)           660         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,686,684\n",
      "Trainable params: 121,184\n",
      "Non-trainable params: 11,565,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# img_file = './model_arch.png'\n",
    "# tf.keras.utils.plot_model(model, to_file=img_file, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class ModelMetric(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,validation_data):\n",
    "        self.x_cv = validation_data[0]\n",
    "        self.y_cv = validation_data[1]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'f1_score': [],'auc': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "        \n",
    "        model_weights = self.model.get_weights()\n",
    "        if model_weights is not None:\n",
    "            if np.any([np.any(np.isnan(x)) for x in model_weights]):\n",
    "                print(\"Invalid weight and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "            \n",
    "        if self.model.stop_training == False:\n",
    "            print(\"Model Metric\")\n",
    "            true_positives=0\n",
    "            ## on end of each epoch, we will get logs and update the self.history dict\n",
    "            self.history['loss'].append(logs.get('loss'))\n",
    "            self.history['accuracy'].append(logs.get('categorical_accuracy'))\n",
    "\n",
    "            if logs.get('val_loss', -1) != -1:\n",
    "                self.history['val_loss'].append(logs.get('val_loss'))\n",
    "            if logs.get('val_accuracy', -1) != -1:\n",
    "                self.history['val_accuracy'].append(logs.get('val_accuracy'))\n",
    "\n",
    "def lr_setter(epoch,lr):\n",
    "    if epoch%3 == 0:\n",
    "        return lr*(0.95)\n",
    "    return lr\n",
    "\n",
    "logs = ModelMetric((X_cv,y_cv))\n",
    "filepath   = \"./model_save/weights-{epoch:02d}-{val_categorical_accuracy:.4f}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_categorical_accuracy',  verbose=1, save_best_only=True, mode='auto')\n",
    "#lrschedule = tf.keras.callbacks.LearningRateScheduler(lr_setter, verbose=1)\n",
    "#reduce     = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',factor=0.9,patience=3,verbose=1)\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.005, patience=5, verbose=1)\n",
    "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "all_callbacks = [checkpoint,logs,tensorboard_callback]# early_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - ETA: 0s - loss: 2.9987 - categorical_accuracy: 0.0481\n",
      "Epoch 1: val_categorical_accuracy improved from 0.05996 to 0.06462, saving model to ./model_save/weights-01-0.0646.hdf5\n",
      "Model Metric\n",
      "53/53 [==============================] - 3s 49ms/step - loss: 2.9987 - categorical_accuracy: 0.0481 - val_loss: 2.9943 - val_categorical_accuracy: 0.0646\n",
      "Epoch 2/10\n",
      "52/53 [============================>.] - ETA: 0s - loss: 2.9940 - categorical_accuracy: 0.0577\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 2.9940 - categorical_accuracy: 0.0575 - val_loss: 2.9935 - val_categorical_accuracy: 0.0633\n",
      "Epoch 3/10\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 2.9939 - categorical_accuracy: 0.0555\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 2.9939 - categorical_accuracy: 0.0551 - val_loss: 2.9932 - val_categorical_accuracy: 0.0566\n",
      "Epoch 4/10\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 2.9927 - categorical_accuracy: 0.0564\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 2.9928 - categorical_accuracy: 0.0561 - val_loss: 2.9926 - val_categorical_accuracy: 0.0553\n",
      "Epoch 5/10\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 2.9919 - categorical_accuracy: 0.0578\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 2.9919 - categorical_accuracy: 0.0574 - val_loss: 2.9923 - val_categorical_accuracy: 0.0566\n",
      "Epoch 6/10\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 2.9920 - categorical_accuracy: 0.0534\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 2.9920 - categorical_accuracy: 0.0534 - val_loss: 2.9919 - val_categorical_accuracy: 0.0533\n",
      "Epoch 7/10\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 2.9910 - categorical_accuracy: 0.0593\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 2.9911 - categorical_accuracy: 0.0594 - val_loss: 2.9914 - val_categorical_accuracy: 0.0566\n",
      "Epoch 8/10\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 2.9907 - categorical_accuracy: 0.0574\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 2.9908 - categorical_accuracy: 0.0581 - val_loss: 2.9912 - val_categorical_accuracy: 0.0566\n",
      "Epoch 9/10\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 2.9898 - categorical_accuracy: 0.0562\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 2.9897 - categorical_accuracy: 0.0567 - val_loss: 2.9909 - val_categorical_accuracy: 0.0606\n",
      "Epoch 10/10\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 2.9889 - categorical_accuracy: 0.0580\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.06462\n",
      "Model Metric\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 2.9888 - categorical_accuracy: 0.0578 - val_loss: 2.9906 - val_categorical_accuracy: 0.0646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147ea813a580>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=10,validation_data=(X_cv,y_cv), batch_size=256,callbacks = all_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0mwdtcvYv1X"
   },
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXPPsovJ3ePk"
   },
   "source": [
    "<pre>\n",
    "<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \n",
    "In the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n",
    " i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n",
    " we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n",
    "<img src='https://i.imgur.com/kiVQuk1.png'>\n",
    "Ref: https://i.imgur.com/kiVQuk1.png\n",
    "\n",
    "<b>Reference:</b>\n",
    "<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n",
    "<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n",
    "\n",
    "<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n",
    "\n",
    "</pre>\n",
    "\n",
    "### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGVQKge3Yv1e"
   },
   "source": [
    "<img src='https://i.imgur.com/fv1GvFJ.png'>\n",
    "ref: 'https://i.imgur.com/fv1GvFJ.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC6SBG5AYv1f"
   },
   "source": [
    "<pre>\n",
    "1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n",
    "\n",
    "2. use concatenate layer is to concatenate all the filters/channels. \n",
    "\n",
    "3. You can use any pool size and stride for maxpooling layer.\n",
    "\n",
    "4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n",
    "( Only recommendation if you have less computing power )\n",
    "\n",
    "5. You can use any number of layers after the Flatten Layer.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cg4L1V4Yv1d"
   },
   "source": [
    "### Model-2 : Using 1D convolutions with character embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Djg4YVA3oQx"
   },
   "source": [
    "<pre>\n",
    "<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\n",
    "Here are the some papers based on Char-CNN\n",
    " 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n",
    " 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n",
    " 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n",
    " 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXvKSEIeSvN5"
   },
   "source": [
    "<img src='https://i.imgur.com/EuuoJtr.png'>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
